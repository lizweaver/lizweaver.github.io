<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project 5: The Power of Diffusion Models - Part A</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@200..900&family=Newsreader:ital,opsz,wght@0,6..72,200..800;1,6..72,200..800&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../styles.css">
    
    <!-- MathJax for mathematical equations -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']]
            }
        };
    </script>
</head>
<body>
    <header>
        <a href="../index.html" style="text-decoration: none;"><h1 class="site-title">Elizabeth Weaver</h1></a>
        <nav>
            <a href="../index.html" class="nav-gray">Home</a>
            <a href="../projects.html" class="nav-dark">Projects</a>
            <a href="../blog.html" class="nav-green">Blog</a>
        </nav>
    </header>

    <div class="ascii-art"></div>

    <div class="breadcrumb">
        <a href="../projects.html">Projects</a> / <a href="index.html">CS180</a> / Project 5: The Power of Diffusion Models - Part A
    </div>

    <main class="main-content projects with-breadcrumb">
        <div class="profile-section">
            <div class="project-header">
                <h1>Part A: The Power of Diffusion Models!</h1>
                <p>
                    In this project, I explore diffusion models using the DeepFloyd IF model. I implement diffusion 
                    sampling loops from scratch, experiment with denoising techniques, and use diffusion models for 
                    creative tasks such as inpainting, image-to-image translation, visual anagrams, and hybrid images. 
                    This project demonstrates the power of modern generative AI models and their applications in 
                    computational photography.
                </p>
            </div>

            <div class="project-section">
                <h2>Part 0: Setup</h2>
                <p>
                    I used the DeepFloyd IF diffusion model, a two-stage model trained by Stability AI. The first stage 
                    produces images of size 64×64, and the second stage upsamples them to 256×256. I accessed the model 
                    through Hugging Face after accepting the usage conditions and logging in with my access token.
                </p>
                <p>
                    To generate images from text prompts, I needed to convert text strings into high-dimensional prompt 
                    embeddings (4096 dimensions). I used Hugging Face clusters to generate embeddings for my custom 
                    text prompts, which I then loaded into the notebook.
                </p>
                <p>
                    I created embeddings for the following prompts:
                </p>
                <ul>
                    <li>"a high quality photo" (used as a baseline/null condition)</li>
                    <li>"a watercolor painting of a wildflower meadow"</li>
                    <li>"a photo of a cat sleeping on a windowsill"</li>
                    <li>"an oil painting of a bustling farmer's market"</li>
                    <li>"a lithograph of a lighthouse on a cliff"</li>
                    <li>"a photo of a vintage bicycle leaning against a brick wall"</li>
                    <li>"a watercolor painting of a hot air balloon"</li>
                    <li>"a sketch of a piano player performing"</li>
                    <li>"a photo of frost on winter branches"</li>
                    <li>"a photo of a cozy library with warm lighting"</li>
                    <li>"a photo of a coffee cup with latte art"</li>
                    <li>"an oil painting of a stormy sea"</li>
                    <li>"a photo of dewdrops on a spider web"</li>
                    <li>"" (empty prompt for unconditional generation)</li>
                </ul>
                <p>
                    I used a consistent random seed throughout all experiments to ensure reproducibility. The random seed 
                    value I used was <strong>42</strong>, which I set at the beginning of the notebook and used consistently 
                    for all subsequent parts.
                </p>
                <p>
                    I chose three of my prompts to generate images and explore how different settings affect the output quality:
                </p>
                <ol>
                    <li><strong>"a watercolor painting of a wildflower meadow"</strong> - An artistic watercolor style prompt</li>
                    <li><strong>"a photo of a cat sleeping on a windowsill"</strong> - A photorealistic subject prompt</li>
                    <li><strong>"an oil painting of a bustling farmer's market"</strong> - An artistic oil painting style prompt</li>
                </ol>
                <p>
                    I experimented with different <code>num_inference_steps</code> values to understand the trade-off 
                    between generation speed and image quality. The <code>num_inference_steps</code> parameter controls how 
                    many denoising steps the model takes: fewer steps are faster but produce lower quality images, while 
                    more steps take longer but generate higher quality results.
                </p>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/part0_numinference_out.jpg" alt="Comparison of different num_inference_steps">
                    <div class="project-caption">Comparison of different num_inference_steps: Generated images for three chosen prompts at different num_inference_steps values (2, 20, and 200). The prompts are: "a watercolor painting of a wildflower meadow" (left column), "a photo of a cat sleeping on a windowsill" (middle column), and "an oil painting of a bustling farmer's market" (right column). The images clearly demonstrate how increasing num_inference_steps dramatically improves image quality—from pure noise at 2 steps, to recognizable but pixelated images at 20 steps, to clear, detailed images at 200 steps.</div>
                </div>
                <p>
                    <strong>Reflections on output quality:</strong> The diffusion model demonstrates impressive capability 
                    in generating images that match text prompts. The quality of outputs varies based on the prompt type—photorealistic 
                    prompts like "a photo of a cat sleeping on a windowsill" produce detailed, realistic images, while artistic 
                    prompts like "a watercolor painting of a wildflower meadow" and "an oil painting of a bustling farmer's market" 
                    generate stylized images that match the artistic medium described. The relationship between prompts and outputs 
                    is strong, with the model successfully interpreting both concrete subjects (cat, market scenes) and artistic 
                    styles (watercolor, oil painting). 
                </p>
                <p>
                    The comparison across different <code>num_inference_steps</code> values reveals a clear progression: at 2 steps, 
                    the images are essentially pure noise with no recognizable content. At 20 steps, basic structure and color 
                    coherence emerge, but images remain heavily pixelated and abstract. At 200 steps, the images become clear, 
                    detailed, and closely match their text descriptions. 
                </p>
                <p>
                    The improvement varies by prompt type. For photorealistic prompts like "a photo of a cat sleeping on a windowsill", 
                    both 20 and 200 steps produce photorealistic results, though 200 steps shows finer detail. For more complex 
                    artistic scenes like "an oil painting of a bustling farmer's market", the difference is more pronounced: at 20 
                    steps the scene is recognizable but lacks the dynamic "bustling" quality, while at 200 steps the model better 
                    captures the energy and activity suggested by the prompt. This demonstrates that while fewer steps are faster, 
                    significantly more steps are required to achieve nuanced interpretations of complex prompts. The improvement is 
                    most dramatic between 2 and 20 steps, with continued refinement up to 200 steps, especially for prompts requiring 
                    complex scene understanding.
                </p>
            </div>

            <div class="project-section">
                <h2>Part 1: Sampling Loops</h2>
                <p>
                    In this section, I implement the core components of diffusion models: the forward process (adding 
                    noise), denoising operations, and iterative sampling loops. These form the foundation for all 
                    subsequent applications.
                </p>
            </div>

            <div class="project-section">
                <h2>Part 1.1: Implementing the Forward Process</h2>
                <p>
                    The forward process is fundamental to diffusion models. It takes a clean image and adds noise to it 
                    according to a predefined schedule. The forward process is defined by:
                </p>
                $$q(x_t | x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t) I)$$
                <p>
                    where $\bar{\alpha}_t$ (alpha_cumprod) controls the amount of noise at timestep $t$. When $t=0$, we 
                    have a clean image, and for larger $t$, more noise is added. The $\bar{\alpha}_t$ values are close 
                    to 1 for small $t$ and close to 0 for large $t$.
                </p>
                <p>
                    I implemented the <code>forward(im, t)</code> function that takes a clean image and a timestep, 
                    then samples a noisy version according to the diffusion schedule. I tested this on a 64×64 resized 
                    image of the Berkeley Campanile at noise levels 250, 500, and 750.
                </p>

                <div class="code-container">
                    <div class="code-title">Forward process implementation (pseudocode)</div>
                    <pre><code>FUNCTION forward(clean_image, timestep):
    """
    Add noise to a clean image according to the diffusion schedule.
    
    Input:
        clean_image: Clean image x₀
        timestep: Integer t from 0 to 1000
    
    Output:
        noisy_image: Noisy image x_t at timestep t
    """
    
    1. GET noise schedule parameter:
       - ᾱ_t = lookup_cumulative_alpha(timestep)
       - This is a precomputed value controlling signal vs noise
    
    2. COMPUTE scaling factors:
       - signal_scale = √(ᾱ_t)
       - noise_scale = √(1 - ᾱ_t)
    
    3. SAMPLE random noise:
       - ε ~ N(0, I)  // Sample Gaussian noise with same shape as image
    
    4. APPLY forward diffusion formula:
       - noisy_image = signal_scale · clean_image + noise_scale · ε
       - This implements: x_t = √(ᾱ_t) · x₀ + √(1 - ᾱ_t) · ε
    
    5. RETURN noisy_image
    
    Key insight: At t=0, ᾱ_t ≈ 1, so noisy_image ≈ clean_image
                 At t=1000, ᾱ_t ≈ 0, so noisy_image ≈ pure noise</code></pre>
                </div>

                <div class="image-container">
                    <img src="project5/proj5a_imgs/campanile_noise_levels.jpg" alt="Campanile at different noise levels">
                    <div class="project-caption">Campanile at different noise levels: The forward process applied to the Campanile at timesteps t=250, t=500, and t=750. As the timestep increases, more noise is added to the image, progressively obscuring the original structure.</div>
                </div>
            </div>

            <div class="project-section">
                <h2>Part 1.2: Classical Denoising</h2>
                <p>
                    Before using the diffusion model, I attempted to denoise the images using classical methods—specifically, 
                    Gaussian blur filtering. This serves as a baseline to demonstrate why diffusion models are necessary. 
                    I applied Gaussian blur to the noisy Campanile images at timesteps 250, 500, and 750.
                </p>
                <p>
                    As expected, Gaussian blur filtering struggles to remove noise while preserving image details. The 
                    results show that simple filtering cannot effectively separate signal from noise, especially at high 
                    noise levels. This motivates the need for learned denoising models.
                </p>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/step250_gauss_blur.jpg" alt="Gaussian blur denoising at t=250">
                    <div class="project-caption">Gaussian blur denoising at t=250 using kernel size 7, 11, 15. The blur reduces noise but also removes important image details. Kernel size 15 almost removes the noise.</div>
                </div>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/step500_gauss_blur.jpg" alt="Gaussian blur denoising at t=500">
                    <div class="project-caption">Gaussian blur denoising at t=500 using kernel size 15, 19, 23. At higher noise levels, Gaussian blur becomes increasingly ineffective. Kernel size 23 almost removes the noise, but the campanile's windows are completely lost.</div>
                </div>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/step750_gauss_blur.jpg" alt="Gaussian blur denoising at t=750">
                    <div class="project-caption">Gaussian blur denoising at t=750 using kernel size 23, 27, 31. The classical method fails to recover meaningful structure from heavily noisy images. Kernel size 31 removes some noise from the sky, but at this point the campanile is extremely hard to make out.</div>
                </div>
            </div>

            <div class="project-section">
                <h2>Part 1.3: One-Step Denoising</h2>
                <p>
                    Now I use the pretrained diffusion model UNet to denoise images. The UNet takes a noisy image and 
                    timestep as input and predicts the noise in the image. With the predicted noise, I can estimate the 
                    clean image.
                </p>
                <p>
                    The key insight is that the UNet was trained on millions of (noisy image, timestep, clean image) 
                    pairs, so it has learned to project noisy images back onto the manifold of natural images. However, 
                    for a single denoising step, the quality degrades as more noise is added, since the problem becomes 
                    increasingly difficult.
                </p>
                <p>
                    I implemented one-step denoising for the three noisy Campanile images (t = 250, 500, 750). The results 
                    show that the diffusion model performs significantly better than Gaussian blur, especially at moderate 
                    noise levels. However, at very high noise levels (t=750), even the diffusion model struggles with 
                    a single step.
                </p>
                <p>
                    For each of the three noisy images (t = [250, 500, 750]), I show the original image, the noisy image, 
                    and the one-step denoised estimate side by side:
                </p>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/denoising_250.png" alt="One-step denoising at t=250">
                    <div class="project-caption">One-step denoising at t=250: Original image (left), noisy image (middle), and denoised estimate (right). The diffusion model successfully recovers most of the structure at this moderate noise level.</div>
                </div>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/denoising_500.png" alt="One-step denoising at t=500">
                    <div class="project-caption">One-step denoising at t=500: Original image (left), noisy image (middle), and denoised estimate (right). At this higher noise level, the estimate shows more artifacts but still captures the main structure.</div>
                </div>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/denoising_750.png" alt="One-step denoising at t=750">
                    <div class="project-caption">One-step denoising at t=750: Original image (left), noisy image (middle), and denoised estimate (right). At this very high noise level, the structure of the campanile is starting to be lost, the top now having more of a dome-like shape.</div>
                </div>
            </div>

            <div class="project-section">
                <h2>Part 1.4: Iterative Denoising</h2>
                <p>
                    Diffusion models are designed to denoise iteratively. Instead of removing all noise in one step, 
                    we gradually reduce noise over multiple steps. This is much more effective than single-step denoising.
                </p>
                <p>
                    To speed up the process, I use strided timesteps—starting at timestep 990 and taking steps of size 
                    30 until reaching 0. This creates a schedule: [990, 960, 930, ..., 30, 0]. The rationale for step 
                    skipping comes from connections with differential equations, allowing us to take larger steps while 
                    maintaining quality.
                </p>
                <p>
                    At each step, I compute the next less-noisy image using the formula:
                </p>
                $$x_{t'} = \sqrt{\bar{\alpha}_{t'}} \hat{x}_0 + \sqrt{1 - \bar{\alpha}_{t'}} \epsilon$$
                <p>
                    where $\hat{x}_0$ is the estimated clean image from one-step denoising, and $\epsilon$ includes 
                    predicted variance for randomness.
                </p>

                <div class="code-container">
                    <div class="code-title">Strided timesteps and iterative denoising (pseudocode)</div>
                    <pre><code># STEP 1: Create strided timesteps
FUNCTION create_strided_timesteps():
    """
    Create a list of timesteps with stride 30, starting at 990, ending at 0.
    This speeds up sampling by skipping intermediate steps.
    """
    
    timesteps = []
    current = 990
    
    WHILE current >= 0:
        timesteps.append(current)
        current = current - 30
    
    // Result: [990, 960, 930, ..., 30, 0]
    
    RETURN timesteps


# STEP 2: Iterative denoising function
FUNCTION iterative_denoise(noisy_image, start_index, prompt_embeddings, timesteps):
    """
    Denoise an image iteratively from timesteps[start_index] to timesteps[0].
    
    Input:
        noisy_image: Noisy image x_t
        start_index: Starting index in timesteps array
        prompt_embeddings: Text prompt embeddings for conditioning
        timesteps: List of timesteps [990, 960, ..., 0]
    
    Output:
        clean_image: Denoised image x₀
        intermediate_images: List of images at each step
    """
    
    current_image = noisy_image
    intermediate_images = []
    
    FOR i FROM start_index TO length(timesteps) - 2:
        
        // Get current and next timesteps
        t = timesteps[i]        // Current timestep (more noisy)
        t' = timesteps[i+1]     // Next timestep (less noisy)
        
        // Get noise schedule parameters
        ᾱ_t = lookup_cumulative_alpha(t)
        ᾱ_t' = lookup_cumulative_alpha(t')
        α_t = ᾱ_t / ᾱ_t'
        β_t = 1 - α_t
        
        // PREDICT NOISE using diffusion model
        model_output = diffusion_model(current_image, t, prompt_embeddings)
        // Model outputs [noise_estimate, variance_estimate]
        noise_estimate = extract_noise_estimate(model_output)
        variance_estimate = extract_variance_estimate(model_output)
        
        // ESTIMATE CLEAN IMAGE from noisy image and predicted noise
        x₀_estimate = (current_image - √(1 - ᾱ_t) · noise_estimate) / √(ᾱ_t)
        
        // COMPUTE COEFFICIENTS for interpolation
        x₀_coeff = √(ᾱ_t') · β_t / (1 - ᾱ_t)
        x_t_coeff = √(α_t) · (1 - ᾱ_t') / (1 - ᾱ_t)
        
        // INTERPOLATE between estimated clean image and current noisy image
        next_image = x₀_coeff · x₀_estimate + x_t_coeff · current_image
        
        // ADD VARIANCE for randomness (prevents mode collapse)
        next_image = add_variance(variance_estimate, t, next_image)
        
        // UPDATE for next iteration
        current_image = next_image
        intermediate_images.append(current_image)
    
    RETURN current_image, intermediate_images</code></pre>
                </div>

                <p>
                    I tested iterative denoising on the Campanile image starting from timestep 90 (i_start=10). The 
                    results show progressive improvement as noise is gradually removed.
                </p>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/iterative_denoise_5_loop.jpg" alt="Iterative denoising progression">
                    <div class="project-caption">Iterative denoising progression showing every 5th step, starting from timestep 90 (i_start=10). The image gradually becomes cleaner as noise is removed iteratively. The final result is much better than one-step denoising or Gaussian blur. From left to right, the timesteps are 690, 540, 390, 240, and 90.</div>
                </div>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/iterative_vs_onestep.jpg" alt="Comparison of denoising methods">
                    <div class="project-caption">Comparison of different denoising methods: original noisy image, iteratively denoised result, one-step denoising (much worse quality), and Gaussian blur filtering (kernel size 5, sigma=2). This demonstrates that iterative denoising produces significantly better results than single-step denoising or classical filtering methods.</div>
                </div>
            </div>

            <div class="project-section">
                <h2>Part 1.5: Diffusion Model Sampling</h2>
                <p>
                    One of the most powerful applications of diffusion models is generating images from scratch. By 
                    starting with pure random noise and iteratively denoising it, we can create entirely new images.
                </p>
                <p>
                    I implemented image generation by setting i_start=0 (starting from pure noise at timestep 990) and 
                    running the iterative denoising process. I generated 5 images using the prompt "a high quality photo" 
                    as a weak condition.
                </p>
                <p>
                    The results show that the model can generate diverse, coherent images from random noise. However, 
                    without stronger conditioning, the images are somewhat generic and lack specific content. This 
                    motivates the use of Classifier-Free Guidance (CFG) in the next section.
                </p>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/sampled_images.jpg" alt="Sampled images without CFG">
                    <div class="project-caption">Sampled images without CFG: Five images sampled from pure noise using the prompt "a high quality photo". The images are diverse but somewhat generic without stronger text conditioning.</div>
                </div>
            </div>

            <div class="project-section">
                <h2>Part 1.6: Classifier-Free Guidance (CFG)</h2>
                <p>
                    Classifier-Free Guidance dramatically improves image quality by using both conditional and 
                    unconditional noise estimates. The formula is:
                </p>
                $$\epsilon = \epsilon_{\text{uncond}} + \gamma (\epsilon_{\text{cond}} - \epsilon_{\text{uncond}})$$
                <p>
                    where $\gamma$ (the CFG scale) controls the strength of guidance. For $\gamma=1$, we get the 
                    conditional estimate. For $\gamma > 1$, we amplify the difference between conditional and 
                    unconditional estimates, leading to higher quality but less diverse images.
                </p>
                <p>
                    I implemented <code>iterative_denoise_cfg</code> which computes both conditional and unconditional 
                    noise estimates at each step. The unconditional estimate uses an empty prompt embedding, while the 
                    conditional uses the text prompt embedding.
                </p>

                <div class="code-container">
                    <div class="code-title">Classifier-Free Guidance implementation (pseudocode)</div>
                    <pre><code>FUNCTION iterative_denoise_cfg(noisy_image, start_index, conditional_prompt, 
                                unconditional_prompt, timesteps, cfg_scale=7):
    """
    Iterative denoising with Classifier-Free Guidance (CFG).
    CFG improves quality by amplifying the difference between conditional 
    and unconditional noise estimates.
    
    Input:
        noisy_image: Noisy image x_t
        start_index: Starting index in timesteps array
        conditional_prompt: Text prompt embeddings (with text)
        unconditional_prompt: Empty prompt embeddings (unconditional)
        timesteps: List of timesteps
        cfg_scale: CFG scale γ, typically 7.0
    
    Output:
        clean_image: Denoised image with CFG
        intermediate_images: List of images at each step
    """
    
    current_image = noisy_image
    intermediate_images = []
    
    FOR i FROM start_index TO length(timesteps) - 2:
        
        t = timesteps[i]
        t' = timesteps[i+1]
        ᾱ_t = lookup_cumulative_alpha(t)
        ᾱ_t' = lookup_cumulative_alpha(t')
        α_t = ᾱ_t / ᾱ_t'
        β_t = 1 - α_t
        
        // COMPUTE CONDITIONAL noise estimate (with text prompt)
        cond_output = diffusion_model(current_image, t, conditional_prompt)
        ε_cond = extract_noise_estimate(cond_output)
        variance = extract_variance_estimate(cond_output)
        
        // COMPUTE UNCONDITIONAL noise estimate (empty prompt)
        uncond_output = diffusion_model(current_image, t, unconditional_prompt)
        ε_uncond = extract_noise_estimate(uncond_output)
        
        // APPLY CFG FORMULA: amplify difference between conditional and unconditional
        ε = ε_uncond + cfg_scale · (ε_cond - ε_uncond)
        // For cfg_scale=7: ε = ε_uncond + 7 · (ε_cond - ε_uncond)
        // This amplifies the effect of the text prompt
        
        // ESTIMATE CLEAN IMAGE using CFG noise estimate
        x₀_estimate = (current_image - √(1 - ᾱ_t) · ε) / √(ᾱ_t)
        
        // COMPUTE INTERPOLATION COEFFICIENTS
        x₀_coeff = √(ᾱ_t') · β_t / (1 - ᾱ_t)
        x_t_coeff = √(α_t) · (1 - ᾱ_t') / (1 - ᾱ_t)
        
        // INTERPOLATE to get next less-noisy image
        next_image = x₀_coeff · x₀_estimate + x_t_coeff · current_image
        
        // ADD VARIANCE (use conditional variance)
        next_image = add_variance(variance, t, next_image)
        
        current_image = next_image
        intermediate_images.append(current_image)
    
    RETURN current_image, intermediate_images</code></pre>
                </div>

                <p>
                    I generated 5 images with CFG scale $\gamma=7$ using the prompt "a high quality photo". The results 
                    show significantly improved quality compared to sampling without CFG.
                </p>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/sampled_images_cfg.jpg" alt="Sampled images with CFG">
                    <div class="project-caption">Sampled images with CFG: Five images sampled with CFG scale γ=7. The images show improved quality and coherence compared to sampling without CFG.</div>
                </div>
            </div>

            <div class="project-section">
                <h2>Part 1.7: Image-to-image Translation</h2>
                <p>
                    Diffusion models can be used to edit existing images through a technique called SDEdit. The idea is 
                    to add a controlled amount of noise to an image, then denoise it. The denoising process "forces" the 
                    noisy image back onto the natural image manifold, effectively making edits.
                </p>
                <p>
                    The amount of noise added controls the strength of the edit: more noise leads to larger changes, 
                    while less noise preserves more of the original. This works because the diffusion model must 
                    "hallucinate" new content to denoise the image, and this creativity enables editing.
                </p>
                <p>
                    I tested SDEdit on the Campanile image using starting indices [1, 3, 5, 7, 10, 20] (corresponding to 
                    different starting points in the strided timesteps). The parameter <code>i_start</code> controls how much 
                    noise is added: lower values (like i_start=1) mean starting from a noisier state, resulting in more 
                    dramatic transformations that look less like the original. Higher values (like i_start=20) mean starting 
                    from a less noisy state, resulting in images that look more similar to the original. The results show 
                    a gradual transition from heavily edited versions (left) to images that closely match the original (right).
                </p>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/campanile_img_to_img.jpg" alt="Campanile SDEdit at different noise levels">
                    <div class="project-caption">Campanile SDEdit at different starting indices [1, 3, 5, 7, 10, 20] (left to right), with the original image on the rightmost side. Lower i_start values (left) correspond to more noise added, creating more dramatic edits that look less like the original. Higher i_start values (right) correspond to less noise, preserving more of the original appearance.</div>
                </div>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/loki_img_to_img.jpg" alt="Loki SDEdit at different noise levels">
                    <div class="project-caption">Loki SDEdit at different starting indices [1, 3, 5, 7, 10, 20] (left to right), with the original image on the rightmost side. Lower i_start values (left) add more noise, resulting in more transformed images. Higher i_start values (right) add less noise, resulting in images that look more similar to the original.</div>
                </div>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/windmill_img_to_img.jpg" alt="Windmill SDEdit at different noise levels">
                    <div class="project-caption">Windmill SDEdit at different starting indices [1, 3, 5, 7, 10, 20] (left to right), with the original image on the rightmost side. The transformation strength decreases from left to right: i_start=1 produces the most dramatic edit (most noise), while i_start=20 produces the most conservative edit (least noise, closest to original).</div>
                </div>
            </div>

            <div class="project-section">
                <h2>Part 1.7.1: Editing Hand-Drawn and Web Images</h2>
                <p>
                    SDEdit works particularly well when starting with non-realistic images (paintings, sketches, drawings) 
                    and projecting them onto the natural image manifold. This allows us to "realize" hand-drawn sketches 
                    or stylized images as photorealistic photos.
                </p>
                <p>
                    I tested this on:
                </p>
                <ol>
                    <li><strong>Web image:</strong> A drawing tutorial image downloaded from the internet</li>
                    <li><strong>Hand-drawn images:</strong> Two sketches I drew using the provided drawing interface</li>
                </ol>
                <p>
                    For each image, I applied SDEdit at starting indices [1, 3, 5, 7, 10, 20] to show the progression from 
                    the original drawing to a more realistic image. Lower i_start values add more noise, creating more 
                    dramatic transformations, while higher values preserve more of the original drawing.
                </p>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/web_img_to_img_all_i_start.jpg" alt="Web image SDEdit">
                    <div class="project-caption">Web image SDEdit: Applied to a web image (drawing tutorial) at different starting indices [1, 3, 5, 7, 10, 20] (left to right), with the original image on the rightmost side. Lower i_start values (left) add more noise, creating more dramatic transformations from stylized drawing to realistic image. Higher i_start values (right) add less noise, preserving more of the original drawing style.</div>
                </div>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/dog_drawing_img_to_img_all_i_start.jpg" alt="Hand-drawn dog SDEdit">
                    <div class="project-caption">Hand-drawn dog SDEdit: Applied to a hand-drawn dog sketch at different starting indices [1, 3, 5, 7, 10, 20] (left to right), with the original image on the rightmost side. Lower i_start values (left) add more noise, resulting in more realistic interpretations. Higher i_start values (right) add less noise, keeping the image closer to the original sketch while still adding realistic details.</div>
                </div>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/mountain_drawing_img_to_img_all_i_start.jpg" alt="Hand-drawn mountain SDEdit">
                    <div class="project-caption">Hand-drawn mountain SDEdit: Applied to a hand-drawn mountain landscape at different starting indices [1, 3, 5, 7, 10, 20] (left to right), with the original image on the rightmost side. Lower i_start values (left) add more noise, creating more dramatic transformations with realistic texture, lighting, and depth. Higher i_start values (right) add less noise, maintaining more of the original drawing structure.</div>
                </div>
            </div>

            <div class="project-section">
                <h2>Part 1.7.2: Inpainting</h2>
                <p>
                    Inpainting is the task of filling in missing or masked regions of an image. I implemented inpainting 
                    using the RePaint algorithm, which runs the diffusion denoising loop but at each step, "forces" the 
                    unmasked regions to match the original image (with appropriate noise added for the current timestep).
                </p>
                <p>
                    The algorithm works by:
                </p>
                <ol>
                    <li>Running the normal denoising loop to get $x_{t'}$</li>
                    <li>Replacing pixels in the unmasked region with the original image (with noise added for timestep $t'$)</li>
                    <li>Repeating until we reach a clean image</li>
                </ol>

                <div class="code-container">
                    <div class="code-title">Inpainting implementation (pseudocode)</div>
                    <pre><code>FUNCTION inpaint(original_image, mask, conditional_prompt, unconditional_prompt, 
                 timesteps, cfg_scale=7):
    """
    Inpaint masked regions of an image using the RePaint algorithm.
    
    Input:
        original_image: Original clean image x₀
        mask: Binary mask (1 = region to inpaint, 0 = keep original)
        conditional_prompt: Text prompt embeddings
        unconditional_prompt: Unconditional embeddings
        timesteps: List of timesteps
        cfg_scale: CFG scale
    
    Output:
        inpainted_image: Image with masked regions filled in
    """
    
    // Start with pure noise
    current_image = sample_gaussian_noise(shape_of(original_image))
    
    FOR i FROM 0 TO length(timesteps) - 2:
        
        t = timesteps[i]
        t' = timesteps[i+1]
        ᾱ_t = lookup_cumulative_alpha(t)
        ᾱ_t' = lookup_cumulative_alpha(t')
        α_t = ᾱ_t / ᾱ_t'
        β_t = 1 - α_t
        
        // DENOISE using CFG (same as iterative_denoise_cfg)
        cond_output = diffusion_model(current_image, t, conditional_prompt)
        uncond_output = diffusion_model(current_image, t, unconditional_prompt)
        
        ε_cond = extract_noise_estimate(cond_output)
        ε_uncond = extract_noise_estimate(uncond_output)
        variance = extract_variance_estimate(cond_output)
        
        ε = ε_uncond + cfg_scale · (ε_cond - ε_uncond)
        
        x₀_estimate = (current_image - √(1 - ᾱ_t) · ε) / √(ᾱ_t)
        x₀_coeff = √(ᾱ_t') · β_t / (1 - ᾱ_t)
        x_t_coeff = √(α_t) · (1 - ᾱ_t') / (1 - ᾱ_t)
        
        next_image = x₀_coeff · x₀_estimate + x_t_coeff · current_image
        next_image = add_variance(variance, t, next_image)
        
        // KEY STEP: FORCE unmasked regions to match original (with noise)
        // Add appropriate noise to original image for timestep t'
        original_with_noise = forward(original_image, t')
        
        // Replace unmasked pixels (where mask == 0) with original_with_noise
        // Keep inpainted pixels (where mask == 1) from denoising
        next_image = mask · next_image + (1 - mask) · original_with_noise
        
        current_image = next_image
    
    RETURN current_image</code></pre>
                </div>

                <p>
                    I tested inpainting on three images:
                </p>
                <ol>
                    <li><strong>Campanile:</strong> Inpainted the top of the tower</li>
                    <li><strong>Window:</strong> Inpainted a rectangular region in the center</li>
                    <li><strong>Person:</strong> Inpainted an elliptical region (face removal)</li>
                </ol>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/inpainted_campanile.jpg" alt="Inpainted Campanile">
                    <div class="project-caption">Inpainted Campanile: The top of the tower was masked and filled in by the diffusion model, creating a plausible completion.</div>
                </div>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/inpainted_window.jpg" alt="Inpainted window">
                    <div class="project-caption">Inpainted window: The rectangular masked region was filled with content that matches the surrounding context.</div>
                </div>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/inpainted_person.jpg" alt="Inpainted person">
                    <div class="project-caption">Inpainted person: The elliptical mask (face region) was filled in, demonstrating the model's ability to generate plausible content in masked areas.</div>
                </div>
            </div>

            <div class="project-section">
                <h2>Part 1.7.3: Text-Conditioned Image-to-image Translation</h2>
                <p>
                    By using specific text prompts instead of the generic "a high quality photo", we can guide the 
                    image-to-image translation process. This combines SDEdit with text conditioning, allowing us to 
                    transform images according to both the original content and the text description.
                </p>
                <p>
                    I tested text-conditioned SDEdit on:
                </p>
                <ol>
                    <li><strong>Campanile:</strong> Transformed using prompts like "a lithograph of a lighthouse on a cliff" and "an oil painting of a stormy sea"</li>
                    <li><strong>Two additional images:</strong> Applied the same technique with various artistic style prompts</li>
                </ol>
                <p>
                    The results show that the images gradually look more like the original (as i_start increases and less 
                    noise is added) but also incorporate elements suggested by the text prompt, creating interesting artistic 
                    transformations. Lower i_start values add more noise, creating more dramatic style transformations, while 
                    higher i_start values add less noise, preserving more of the original image structure.
                </p>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/text_conditioned_campanile_denoising.jpg" alt="Text-conditioned Campanile">
                    <div class="project-caption">Text-conditioned Campanile: SDEdit using the prompt "a lithograph of a lighthouse on a cliff" at starting indices [1, 3, 5, 7, 10, 20] (left to right), with the original image on the rightmost side. Lower i_start values (left) add more noise, creating more dramatic transformations to match the artistic style. Higher i_start values (right) add less noise, preserving more of the original Campanile image.</div>
                </div>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/sea_text_conditioned_campanile_denoising.jpg" alt="Stormy sea Campanile">
                    <div class="project-caption">Stormy sea Campanile: Text-conditioned SDEdit using the prompt "an oil painting of a stormy sea" at starting indices [1, 3, 5, 7, 10, 20] (left to right), with the original image on the rightmost side. Lower i_start values (left) add more noise, creating more dramatic, painterly transformations. Higher i_start values (right) add less noise, resulting in subtler style changes that preserve more of the original image.</div>
                </div>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/text_conditioned_person_denoising.jpg" alt="Text-conditioned person">
                    <div class="project-caption">Text-conditioned person: SDEdit using the prompt "a photo of a coffee cup with latte art" at starting indices [1, 3, 5, 7, 10, 20] (left to right), with the original image on the rightmost side. Lower i_start values (left) add more noise, creating more dramatic style transformations. Higher i_start values (right) add less noise, resulting in subtler style changes that preserve more of the original image.</div>
                </div>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/text_conditioned_rose_denoising.jpg" alt="Text-conditioned rose">
                    <div class="project-caption">Text-conditioned rose: SDEdit using the prompt "a photo of a cozy library with warm lighting" at starting indices [1, 3, 5, 7, 10, 20] (left to right), with the original image on the rightmost side. Lower i_start values (left) add more noise, creating more dramatic transformations guided by the prompt. Higher i_start values (right) add less noise, resulting in subtler style changes that preserve more of the original image.</div>
                </div>
            </div>

            <div class="project-section">
                <h2>Part 1.8: Visual Anagrams</h2>
                <p>
                    Visual anagrams are optical illusions where an image looks like one thing when viewed normally, but 
                    reveals a different image when flipped upside down. I implemented this using a technique that 
                    averages noise estimates from two different prompts—one for the normal orientation and one for the 
                    flipped orientation.
                </p>
                <p>
                    The algorithm:
                </p>
                <ol>
                    <li>Denoises the image normally with prompt $p_1$ to get noise estimate $\epsilon_1$</li>
                    <li>Flips the image, denoises with prompt $p_2$ to get $\epsilon_2$, then flips back</li>
                    <li>Averages the two noise estimates: $\epsilon = \frac{1}{2}(\epsilon_1 + \epsilon_2)$</li>
                    <li>Performs the denoising step with the averaged estimate</li>
                </ol>

                <div class="code-container">
                    <div class="code-title">Visual anagrams implementation (pseudocode)</div>
                    <pre><code>FUNCTION visual_anagrams(starting_image, start_index, prompt1, prompt2,
                         unconditional_prompt, timesteps, cfg_scale=7):
    """
    Create a visual anagram: an image that looks like prompt1 when normal,
    but like prompt2 when flipped upside down.
    
    Input:
        starting_image: Starting image (can be noise or noisy image)
        start_index: Starting index in timesteps
        prompt1: Embeddings for normal orientation prompt
        prompt2: Embeddings for flipped orientation prompt
        unconditional_prompt: Unconditional embeddings
        timesteps: List of timesteps
        cfg_scale: CFG scale
    
    Output:
        anagram_image: Image that creates the anagram effect
    """
    
    current_image = starting_image
    
    FOR i FROM start_index TO length(timesteps) - 2:
        
        t = timesteps[i]
        t' = timesteps[i+1]
        ᾱ_t = lookup_cumulative_alpha(t)
        ᾱ_t' = lookup_cumulative_alpha(t')
        α_t = ᾱ_t / ᾱ_t'
        β_t = 1 - α_t
        
        // COMPUTE NOISE ESTIMATE 1: Normal orientation with prompt1
        cond_output_1 = diffusion_model(current_image, t, prompt1)
        uncond_output_1 = diffusion_model(current_image, t, unconditional_prompt)
        
        ε_1_cond = extract_noise_estimate(cond_output_1)
        ε_1_uncond = extract_noise_estimate(uncond_output_1)
        variance = extract_variance_estimate(cond_output_1)
        
        ε_1 = ε_1_uncond + cfg_scale · (ε_1_cond - ε_1_uncond)
        
        // COMPUTE NOISE ESTIMATE 2: Flipped orientation with prompt2
        flipped_image = flip_vertically(current_image)
        
        cond_output_2 = diffusion_model(flipped_image, t, prompt2)
        uncond_output_2 = diffusion_model(flipped_image, t, unconditional_prompt)
        
        ε_2_cond = extract_noise_estimate(cond_output_2)
        ε_2_uncond = extract_noise_estimate(uncond_output_2)
        ε_2_flipped = ε_2_uncond + cfg_scale · (ε_2_cond - ε_2_uncond)
        
        // FLIP BACK to normal orientation
        ε_2 = flip_vertically(ε_2_flipped)
        
        // AVERAGE the two noise estimates
        ε = 0.5 · ε_1 + 0.5 · ε_2
        
        // DENOISE using averaged noise estimate
        x₀_estimate = (current_image - √(1 - ᾱ_t) · ε) / √(ᾱ_t)
        x₀_coeff = √(ᾱ_t') · β_t / (1 - ᾱ_t)
        x_t_coeff = √(α_t) · (1 - ᾱ_t') / (1 - ᾱ_t)
        
        next_image = x₀_coeff · x₀_estimate + x_t_coeff · current_image
        next_image = add_variance(variance, t, next_image)
        
        current_image = next_image
    
    RETURN current_image</code></pre>
                </div>

                <p>
                    I created two visual anagrams:
                </p>
                <ol>
                    <li><strong>Balloon/Lighthouse:</strong> "a watercolor painting of a hot air balloon" (normal) and "a lithograph of a lighthouse on a cliff" (flipped)</li>
                    <li><strong>Farmers/Sea:</strong> "an oil painting of a bustling farmer's market" (normal) and "an oil painting of a stormy sea" (flipped)</li>
                </ol>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/balloon_lighthouse_anagram.jpg" alt="Balloon lighthouse anagram">
                    <div class="project-caption">Balloon lighthouse anagram: When viewed normally, the image shows "a watercolor painting of a hot air balloon". When flipped upside down, it reveals "a lithograph of a lighthouse on a cliff".</div>
                </div>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/farmers_sea_anagram.jpg" alt="Farmers sea anagram">
                    <div class="project-caption">Farmers sea anagram: Normal orientation shows "an oil painting of a bustling farmer's market", while the flipped version reveals "an oil painting of a stormy sea".</div>
                </div>
            </div>

            <div class="project-section">
                <h2>Part 1.9: Hybrid Images</h2>
                <p>
                    Hybrid images combine two different prompts by mixing their frequency components, similar to the 
                    hybrid images from Project 2. I implemented Factorized Diffusion, which creates a composite noise 
                    estimate by combining low frequencies from one prompt with high frequencies from another.
                </p>
                <p>
                    The algorithm:
                </p>
                <ol>
                    <li>Compute noise estimates $\epsilon_1$ and $\epsilon_2$ for prompts $p_1$ and $p_2$</li>
                    <li>Apply Gaussian blur to get low-pass versions: $\epsilon_{1,\text{low}}$, $\epsilon_{2,\text{low}}$</li>
                    <li>Compute high-pass: $\epsilon_{2,\text{high}} = \epsilon_2 - \epsilon_{2,\text{low}}$</li>
                    <li>Combine: $\epsilon = \epsilon_{1,\text{low}} + \epsilon_{2,\text{high}}$</li>
                </ol>

                <div class="code-container">
                    <div class="code-title">Hybrid images implementation (pseudocode)</div>
                    <pre><code>FUNCTION make_hybrids(starting_image, start_index, prompt1, prompt2,
                     unconditional_prompt, timesteps, cfg_scale=7):
    """
    Create hybrid images using Factorized Diffusion.
    Combines low frequencies from prompt1 with high frequencies from prompt2.
    
    Input:
        starting_image: Starting image (typically noise)
        start_index: Starting index in timesteps
        prompt1: Embeddings for low-frequency prompt (visible from far)
        prompt2: Embeddings for high-frequency prompt (visible up close)
        unconditional_prompt: Unconditional embeddings
        timesteps: List of timesteps
        cfg_scale: CFG scale
    
    Output:
        hybrid_image: Hybrid image combining both prompts
    """
    
    current_image = starting_image
    
    FOR i FROM start_index TO length(timesteps) - 2:
        
        t = timesteps[i]
        t' = timesteps[i+1]
        ᾱ_t = lookup_cumulative_alpha(t)
        ᾱ_t' = lookup_cumulative_alpha(t')
        α_t = ᾱ_t / ᾱ_t'
        β_t = 1 - α_t
        
        // COMPUTE NOISE ESTIMATE 1: For prompt1 (low frequencies)
        cond_output_1 = diffusion_model(current_image, t, prompt1)
        uncond_output_1 = diffusion_model(current_image, t, unconditional_prompt)
        
        ε_1_cond = extract_noise_estimate(cond_output_1)
        ε_1_uncond = extract_noise_estimate(uncond_output_1)
        variance = extract_variance_estimate(cond_output_1)
        
        ε_1 = ε_1_uncond + cfg_scale · (ε_1_cond - ε_1_uncond)
        
        // COMPUTE NOISE ESTIMATE 2: For prompt2 (high frequencies)
        cond_output_2 = diffusion_model(current_image, t, prompt2)
        uncond_output_2 = diffusion_model(current_image, t, unconditional_prompt)
        
        ε_2_cond = extract_noise_estimate(cond_output_2)
        ε_2_uncond = extract_noise_estimate(uncond_output_2)
        
        ε_2 = ε_2_uncond + cfg_scale · (ε_2_cond - ε_2_uncond)
        
        // FREQUENCY DECOMPOSITION using Gaussian blur
        // Low-pass filter: blur removes high frequencies
        ε_1_low = apply_gaussian_blur(ε_1, kernel_size=33, sigma=2)
        ε_2_low = apply_gaussian_blur(ε_2, kernel_size=33, sigma=2)
        
        // High-pass filter: subtract low-pass from original
        ε_2_high = ε_2 - ε_2_low
        
        // COMBINE: low frequencies from prompt1, high frequencies from prompt2
        ε = ε_1_low + ε_2_high
        
        // DENOISE using composite noise estimate
        x₀_estimate = (current_image - √(1 - ᾱ_t) · ε) / √(ᾱ_t)
        x₀_coeff = √(ᾱ_t') · β_t / (1 - ᾱ_t)
        x_t_coeff = √(α_t) · (1 - ᾱ_t') / (1 - ᾱ_t)
        
        next_image = x₀_coeff · x₀_estimate + x_t_coeff · current_image
        next_image = add_variance(variance, t, next_image)
        
        current_image = next_image
    
    RETURN current_image</code></pre>
                </div>

                <p>
                    The result is an image that looks like prompt $p_1$ when viewed from far away (low frequencies 
                    dominate) and like prompt $p_2$ when viewed up close (high frequencies dominate).
                </p>
                <p>
                    I created three hybrid images:
                </p>
                <ol>
                    <li><strong>Cat/Meadow:</strong> "a photo of a cat sleeping on a windowsill" (low-pass) and "a watercolor painting of a wildflower meadow" (high-pass)</li>
                    <li><strong>Piano Player/Frost:</strong> "a sketch of a piano player performing" (low-pass) and "a photo of frost on winter branches" (high-pass)</li>
                    <li><strong>Cat/Lighthouse:</strong> "a photo of a cat sleeping on a windowsill" (low-pass) and "a lithograph of a lighthouse on a cliff" (high-pass)</li>
                </ol>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/cat_meadow_hybrid.jpg" alt="Cat meadow hybrid">
                    <div class="project-caption">Cat meadow hybrid: Combining "a photo of a cat sleeping on a windowsill" (low frequencies, visible from far) and "a watercolor painting of a wildflower meadow" (high frequencies, visible up close).</div>
                </div>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/frozen_branch_piano_player.jpg" alt="Piano player frost hybrid">
                    <div class="project-caption">Piano player frost hybrid: Combining "a sketch of a piano player performing" (low frequencies, visible from far away) and "a photo of frost on winter branches" (high frequencies, visible up close). The hybrid image shows the piano player when viewed from a distance, but reveals intricate frost patterns when viewed closely.</div>
                </div>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/cat_lighthouse_hybrid.jpg" alt="Cat lighthouse hybrid">
                    <div class="project-caption">Cat lighthouse hybrid: Combining "a photo of a cat sleeping on a windowsill" (low frequencies) and "a lithograph of a lighthouse on a cliff" (high frequencies).</div>
                </div>
            </div>
            <div class="project-section">
                <div class="project-header">
                    <h1>Part B: Flow Matching from Scratch (MNIST)</h1>
                </div>
                <p>
                    For Part B, I implemented and trained UNet-based denoisers and flow-matching models from scratch on the 
                    MNIST dataset. This project involved building the UNet architecture, implementing time and class conditioning, 
                    and training models to learn flow fields that transform noise into realistic digit images. All experiments 
                    were conducted using PyTorch on Google Colab with GPU acceleration.
                </p>

                <h3>Part 1: Single-Step Denoising UNet</h3>
                <p>
                    I first implemented an unconditional UNet architecture to perform single-step denoising. The UNet consists 
                    of an encoder-decoder structure with skip connections, allowing it to preserve spatial details while learning 
                    hierarchical features. The architecture includes:
                </p>
                <ul>
                    <li><strong>Basic operations:</strong> Conv (3×3 conv + BatchNorm + GELU), DownConv (stride-2 downsampling), 
                        UpConv (stride-2 upsampling), Flatten (7×7→1×1 via AvgPool), Unflatten (1×1→7×7 via ConvTranspose)</li>
                    <li><strong>Composed blocks:</strong> ConvBlock (two Conv layers), DownBlock (DownConv + ConvBlock), 
                        UpBlock (UpConv + ConvBlock)</li>
                    <li><strong>Architecture:</strong> Input → ConvBlock → DownBlock → DownBlock → Flatten → Unflatten → 
                        UpBlock → UpBlock → ConvBlock → Output, with skip connections at each resolution level</li>
                </ul>
                <div class="image-container">
                    <img src="project5/project5b_images/Screenshot 2025-12-05 at 1.09.38 PM.png" alt="UNet architecture diagram">
                    <div class="project-caption">Unconditional UNet architecture showing the encoder-decoder structure with skip connections. The network downsamples through two DownBlocks, flattens to a 1×1 representation, then upsamples back with skip connections preserving spatial details.</div>
                </div>
                <div class="image-container">
                    <img src="project5/project5b_images/Screenshot 2025-12-05 at 1.09.45 PM.png" alt="UNet operations diagram">
                    <div class="project-caption">Standard UNet operations: Conv (convolutional layer), DownConv (downsampling), UpConv (upsampling), Flatten (average pooling to 1×1), Unflatten (conv transpose to 7×7), and Concat (channel-wise concatenation for skip connections).</div>
                </div>
                <p>
                    The training objective was to minimize the L2 loss between the denoised output and the clean image:
                </p>
                $$L = \mathbb{E}_{z, x} \|D_{\theta}(z) - x\|^2$$
                <p>
                    where $z = x + \sigma \epsilon$ is the noisy image ($\sigma = 0.5$), $x$ is the clean image, and 
                    $D_{\theta}$ is the UNet denoiser. I used Adam optimizer with learning rate $10^{-4}$, batch size 256, 
                    and trained for 5 epochs. The noise was added dynamically during training (different $\epsilon$ each epoch) 
                    to improve generalization.
                </p>

                <div class="code-container">
                    <div class="code-title">Single-step denoising training (pseudocode)</div>
                    <pre><code>FUNCTION train_single_step_denoiser(unet, train_loader, num_epochs, noise_level=0.5):
    """
    Train a UNet to denoise noisy images in a single step.
    
    Input:
        unet: UnconditionalUNet model
        train_loader: DataLoader for training images
        num_epochs: Number of training epochs
        noise_level: Standard deviation σ for noise (default 0.5)
    
    Output:
        trained_unet: Trained denoiser model
        train_losses: List of training losses
    """
    
    optimizer = Adam(unet.parameters(), lr=1e-4)
    criterion = MSELoss()
    train_losses = []
    
    FOR epoch IN range(num_epochs):
        FOR batch IN train_loader:
            // Get clean images from batch
            x = batch.images  // Clean images x
            
            // ADD NOISE dynamically (different ε each epoch)
            ε = sample_gaussian_noise(shape=x.shape)  // ε ~ N(0, I)
            z = x + noise_level * ε  // Noisy image z = x + σ·ε
            
            // PREDICT denoised image
            x_pred = unet(z)  // D_θ(z)
            
            // COMPUTE L2 loss
            loss = criterion(x_pred, x)  // ||D_θ(z) - x||²
            
            // BACKPROPAGATE and update
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            train_losses.append(loss.item())
    
    RETURN unet, train_losses
    
    Key insight: Noise is added dynamically during training (different ε each epoch)
                 to improve generalization. The model learns to map noisy images
                 back to clean images via L2 regression.</code></pre>
                </div>
                <div class="image-container">
                    <img src="project5/project5b_images/visualizing_noising_process_all_sigmas.jpg" alt="Noising process across sigmas">
                    <div class="project-caption">Noising process visualization across different σ values.</div>
                </div>
                <div class="image-container">
                    <img src="project5/project5b_images/part121_train_loss.jpg" alt="Training loss for single-step denoiser">
                    <div class="project-caption">Training loss curve for the single-step denoiser (5 epochs, batch size 256, Adam 1e-4).</div>
                </div>
                <div class="image-container">
                    <img src="project5/project5b_images/part121_test_set_epoch0_results.jpg" alt="Epoch 1 denoising results">
                    <div class="project-caption">Test-set denoising results with noise level 0.5 after epoch 1.</div>
                </div>
                <div class="image-container">
                    <img src="project5/project5b_images/part121_test_set_epoch4_results.jpg" alt="Epoch 5 denoising results">
                    <div class="project-caption">Test-set denoising results with noise level 0.5 after epoch 5.</div>
                </div>
                <div class="image-container">
                    <img src="project5/project5b_images/part122_ood_test_results.jpg" alt="OOD sigma denoising results">
                    <div class="project-caption">Out-of-distribution σ sweep on the trained denoiser, holding the same test digits and varying σ. The model was trained only on σ=0.5, so this tests generalization to different noise levels. Performance degrades for very high (σ=0.8, 1.0) noise levels, as expected for a model trained on a specific noise level.</div>
                </div>
                <p>
                    <strong>Out-of-distribution analysis:</strong> The model was trained exclusively on noise level $\sigma = 0.5$. 
                    When tested on different noise levels ($\sigma \in \{0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0\}$), performance 
                    degrades for noise levels far from the training distribution. At very low noise ($\sigma = 0.0, 0.2$), the 
                    model seems to perform just as well as at the training noise level, however at $\sigma = 0.4$, the model introduces 
                    a bit of a squiggly line in the 7's digit. At very high noise ($\sigma = 0.8, 1.0$), the model struggles 
                    to recover structure. This demonstrates the importance of matching the training and inference noise distributions, 
                    or using a more flexible approach like flow matching that handles multiple noise levels.
                </p>
                <h4>Part 1.2.3: Denoising Pure Noise</h4>
                <p>
                    To explore whether single-step denoising can work as a generative model, I trained a separate UNet to denoise 
                    pure Gaussian noise ($z = \epsilon \sim \mathcal{N}(0, I)$) and predict clean images. The training objective 
                    remains the same L2 loss, but now the input is always pure noise rather than a noisy version of a clean image.
                </p>
                <div class="image-container">
                    <img src="project5/project5b_images/part123_train_loss.jpg" alt="Training loss when denoising pure noise">
                    <div class="project-caption">Training loss when training the denoiser on pure Gaussian noise inputs (5 epochs).</div>
                </div>
                <div class="image-container">
                    <img src="project5/project5b_images/part123_test_set_epoch0_results.jpg" alt="Pure noise epoch 1 outputs">
                    <div class="project-caption">Generated samples from the pure-noise denoiser after epoch 1.</div>
                </div>
                <div class="image-container">
                    <img src="project5/project5b_images/part123_test_set_epoch4_results.jpg" alt="Pure noise epoch 5 outputs">
                    <div class="project-caption">Generated samples from the pure-noise denoiser after epoch 5. Outputs cluster around digit-like centroids, consistent with the MSE objective pulling toward the dataset mean.</div>
                </div>
                <p>
                    <strong>Analysis of pure noise denoising:</strong> When training the denoiser on pure Gaussian noise 
                    ($z = \epsilon \sim \mathcal{N}(0, I)$), the model learns to predict the mean of the training distribution. 
                    With an MSE loss, the optimal prediction is the centroid (average) of all training images. This explains why 
                    the outputs appear as blurry, digit-like shapes that represent the "average digit" across all classes—the 
                    model is essentially learning to predict the dataset mean rather than generating specific digits. This 
                    demonstrates why single-step denoising is insufficient for generative tasks; we need iterative denoising 
                    with time conditioning to generate diverse, high-quality samples.
                </p>

                <h3>Part 2: Flow Matching</h3>
                <p>
                    To enable iterative generation from noise, I implemented flow matching models that learn flow fields to 
                    transform noise into realistic images. The models predict the "flow" (velocity vector) that moves from 
                    the noisy state toward the clean state, enabling iterative denoising via Euler integration.
                </p>

                <h4>Part 2.1: Adding Time Conditioning to UNet</h4>
                <p>
                    I first implemented a time-conditioned UNet that learns a flow field. Instead of directly predicting the 
                    clean image, the model predicts the flow that moves from the noisy state toward the clean state. The key 
                    innovation is conditioning the UNet on a normalized timestep $t \in [0, 1]$, where $t=0$ corresponds to 
                    pure noise and $t=1$ corresponds to clean data.
                </p>
                <p>
                    <strong>Time conditioning mechanism:</strong> I added two FCBlock modules (fully-connected blocks with two 
                    linear layers and GELU activations) to inject the timestep $t$ into the UNet. The conditioning is applied 
                    via element-wise multiplication at two locations: after the unflatten operation and after the first upsampling 
                    block. Specifically:
                </p>
                <ul>
                    <li>$t_1 = \text{FCBlock}_1(t)$ is applied to the unflattened feature map: $\text{unflatten} = \text{unflatten} \odot t_1$</li>
                    <li>$t_2 = \text{FCBlock}_2(t)$ is applied after the first upsampling: $\text{up}_1 = \text{up}_1 \odot t_2$</li>
                </ul>
                <div class="image-container">
                    <img src="project5/project5b_images/Screenshot 2025-12-05 at 1.09.54 PM.png" alt="Time-conditioned UNet architecture">
                    <div class="project-caption">Time-conditioned UNet architecture showing the FCBlock modules (fc1_t and fc2_t) that inject the normalized timestep $t$ into the network via element-wise multiplication at the unflatten and up1 stages.</div>
                </div>
                <div class="image-container">
                    <img src="project5/project5b_images/Screenshot 2025-12-05 at 1.10.03 PM.png" alt="FCBlock architecture">
                    <div class="project-caption">FCBlock (fully-connected block) architecture used for conditioning. It consists of two linear layers with GELU activation, transforming the conditioning signal (scalar $t$ or one-hot vector $c$) into feature maps that modulate the UNet activations.</div>
                </div>
                <h4>Part 2.2: Training the Time-Conditioned UNet</h4>
                <p>
                    <strong>Flow matching objective:</strong> During training, I sample a random timestep $t \sim \mathcal{U}(0, 1)$ 
                    and create an intermediate noisy image via linear interpolation:
                </p>
                $$x_t = (1-t) \cdot \epsilon + t \cdot x_1$$
                <p>
                    where $\epsilon \sim \mathcal{N}(0, I)$ is pure noise and $x_1$ is a clean training image. The ground-truth 
                    flow is $v = x_1 - \epsilon$, and the model predicts $\hat{v} = u_{\theta}(x_t, t)$. The loss is:
                </p>
                $$L = \mathbb{E}_{x_1, \epsilon, t} \|u_{\theta}(x_t, t) - (x_1 - \epsilon)\|^2$$

                <div class="code-container">
                    <div class="code-title">Flow matching training (pseudocode)</div>
                    <pre><code>FUNCTION train_flow_matching(time_conditioned_unet, train_loader, num_epochs):
    """
    Train a time-conditioned UNet to predict flow fields for iterative denoising.
    
    Input:
        time_conditioned_unet: TimeConditionalUNet model
        train_loader: DataLoader for training images
        num_epochs: Number of training epochs
    
    Output:
        trained_unet: Trained flow matching model
        train_losses: List of training losses
    """
    
    optimizer = Adam(time_conditioned_unet.parameters(), lr=1e-2)
    scheduler = ExponentialLR(optimizer, gamma=0.1^(1/10))
    criterion = MSELoss()
    train_losses = []
    
    FOR epoch IN range(num_epochs):
        FOR batch IN train_loader:
            // Get clean images from batch
            x_1 = batch.images  // Clean images x₁
            
            // SAMPLE random timestep t ~ U(0, 1)
            N = x_1.shape[0]
            t = sample_uniform(N, min=0, max=1)  // t ~ U(0, 1)
            t_normalized = t.view(N, 1)  // Reshape for UNet input
            
            // SAMPLE noise
            ε = sample_gaussian_noise(shape=x_1.shape)  // ε ~ N(0, I)
            
            // CREATE intermediate noisy image via linear interpolation
            // x_t = (1-t)·ε + t·x₁
            x_t = (1 - t.view(N, 1, 1, 1)) * ε + t.view(N, 1, 1, 1) * x_1
            
            // COMPUTE ground-truth flow
            v_true = x_1 - ε  // Flow from noise to clean image
            
            // PREDICT flow using time-conditioned UNet
            v_pred = time_conditioned_unet(x_t, t_normalized)  // u_θ(x_t, t)
            
            // COMPUTE L2 loss
            loss = criterion(v_pred, v_true)  // ||u_θ(x_t, t) - (x₁ - ε)||²
            
            // BACKPROPAGATE and update
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            train_losses.append(loss.item())
        
        scheduler.step()  // Decay learning rate after each epoch
    
    RETURN time_conditioned_unet, train_losses
    
    Key insight: The model learns to predict the velocity vector (flow) that moves
                 from any intermediate noisy state x_t toward the clean image x₁.
                 This enables iterative generation via Euler integration.</code></pre>
                </div>

                <h4>Part 2.3: Sampling from the Time-Conditioned UNet</h4>
                <p>
                    <strong>Sampling algorithm:</strong> To generate images, I start with pure noise $x_0 = \epsilon$ and iteratively 
                    update using Euler integration:
                </p>
                $$x_{t+\Delta t} = x_t + \Delta t \cdot u_{\theta}(x_t, t)$$
                <p>
                    where I use 50 timesteps ($\Delta t = 1/50$) to integrate from $t=0$ to $t=1$.
                </p>

                <div class="code-container">
                    <div class="code-title">Flow matching sampling (pseudocode)</div>
                    <pre><code>FUNCTION sample_flow_matching(time_conditioned_unet, img_wh, num_ts=50, seed=0):
    """
    Generate images by iteratively denoising from pure noise using flow matching.
    
    Input:
        time_conditioned_unet: Trained TimeConditionalUNet model
        img_wh: (height, width) tuple for output image size
        num_ts: Number of timesteps for Euler integration (default 50)
        seed: Random seed for reproducibility
    
    Output:
        generated_image: Generated image x₁
    """
    
    set_random_seed(seed)
    device = get_device(time_conditioned_unet)
    
    // START with pure noise
    x_t = sample_gaussian_noise(shape=(1, 1, img_wh[0], img_wh[1]), device=device)
    // x₀ = ε ~ N(0, I)
    
    // CREATE timestep schedule: t from 0 to 1
    timesteps = linspace(0, 1, num_ts + 1)  // [0, Δt, 2Δt, ..., 1]
    
    // ITERATIVE denoising via Euler integration
    FOR i IN range(num_ts):
        t = timesteps[i]  // Current timestep
        t_next = timesteps[i + 1]  // Next timestep
        dt = t_next - t  // Step size for this iteration
        
        // NORMALIZE timestep for UNet input
        t_input = t.view(1, 1)
        
        // PREDICT flow at current state
        v_pred = time_conditioned_unet(x_t, t_input)  // u_θ(x_t, t)
        
        // UPDATE state using Euler integration
        // x_{t+dt} = x_t + dt · u_θ(x_t, t)
        x_t = x_t + dt * v_pred
    
    // x_t now contains the generated image x₁
    RETURN x_t
    
    Key insight: Starting from pure noise, we iteratively follow the learned flow
                 field to gradually transform noise into a realistic image. The flow
                 field guides the transformation at each timestep.</code></pre>
                </div>
                <p>
                    <strong>Training details:</strong> I used Adam optimizer with initial learning rate $10^{-2}$, exponential 
                    learning rate decay with $\gamma = 0.1^{1/10} \approx 0.794$ (decaying by factor of 0.1 over 10 epochs), 
                    batch size 64, hidden dimension $D=64$, and trained for 10 epochs.
                </p>
                <div class="image-container">
                    <img src="project5/project5b_images/part22_time_conditioned_train_loss.jpg" alt="Time-conditioned training loss">
                    <div class="project-caption">Training loss for the time-conditioned UNet over the full schedule.</div>
                </div>
                <div class="image-container">
                    <img src="project5/project5b_images/part23_time_conditioned_samples.jpg" alt="Time-conditioned sampling results">
                    <div class="project-caption">Sampling results from the time-conditioned UNet after 1, 5, and 10 epochs. Digits become progressively more legible as training proceeds.</div>
                </div>

                <h3>Part 2.4-2.6: Class-Conditioned Flow Matching UNet</h3>
                <p>
                    To improve generation quality and enable class-specific control, I extended the time-conditioned UNet with 
                    class conditioning. The model now takes both a timestep $t$ and a class label $c \in \{0, 1, \ldots, 9\}$ 
                    (representing digit classes) as inputs.
                </p>

                <h4>Part 2.4: Adding Class-Conditioning to UNet</h4>
                <p>
                    <strong>Class conditioning mechanism:</strong> I added two additional FCBlock modules for class conditioning, 
                    similar to the time conditioning. The class label is converted to a one-hot vector $c_{\text{onehot}} \in \mathbb{R}^{10}$. 
                    The conditioning is applied via a combination of multiplication and addition:
                </p>
                <ul>
                    <li>After unflatten: $\text{unflatten} = c_1 \odot \text{unflatten} + t_1$</li>
                    <li>After first upsampling: $\text{up}_1 = c_2 \odot \text{up}_1 + t_2$</li>
                </ul>
                <p>
                    where $c_1 = \text{FCBlock}_{c1}(c_{\text{onehot}})$ and $c_2 = \text{FCBlock}_{c2}(c_{\text{onehot}})$.
                </p>
                <p>
                    <strong>Classifier-free guidance:</strong> To enable unconditional generation and improve quality via guidance, 
                    I implemented classifier-free guidance (CFG) similar to Part A. During training, with probability $p_{\text{uncond}} = 0.1$, 
                    I set the class conditioning vector to zero (mask it out), allowing the model to learn both conditional and 
                    unconditional flows. During sampling, I use CFG with scale $\gamma = 5.0$:
                </p>
                $$u = u_{\text{uncond}} + \gamma \cdot (u_{\text{cond}} - u_{\text{uncond}})$$
                <p>
                    where $u_{\text{uncond}}$ is the flow with $c=0$ (unconditional) and $u_{\text{cond}}$ is the flow with the 
                    target class $c$.
                </p>

                <h4>Part 2.5: Training the Class-Conditioned UNet</h4>
                <p>
                    <strong>Training details:</strong> I used the same training setup as the time-conditioned model: Adam optimizer 
                    with initial LR $10^{-2}$, exponential LR decay ($\gamma = 0.1^{1/10}$), batch size 64, hidden dimension $D=64$, 
                    and trained for 10 epochs. During sampling, I used 300 timesteps for higher quality generation.
                </p>

                <div class="code-container">
                    <div class="code-title">Class-conditioned flow matching training (pseudocode)</div>
                    <pre><code>FUNCTION train_class_conditioned_flow_matching(class_unet, train_loader, num_epochs, p_uncond=0.1):
    """
    Train a class-conditioned UNet to predict flow fields with classifier-free guidance.
    
    Input:
        class_unet: ClassConditionalUNet model
        train_loader: DataLoader for (images, labels) pairs
        num_epochs: Number of training epochs
        p_uncond: Probability of dropping class conditioning (default 0.1)
    
    Output:
        trained_unet: Trained class-conditioned flow matching model
        train_losses: List of training losses
    """
    
    optimizer = Adam(class_unet.parameters(), lr=1e-2)
    scheduler = ExponentialLR(optimizer, gamma=0.1^(1/10))
    criterion = MSELoss()
    train_losses = []
    
    FOR epoch IN range(num_epochs):
        FOR batch IN train_loader:
            // Get clean images and class labels from batch
            x_1 = batch.images  // Clean images x₁
            c = batch.labels  // Class labels c ∈ {0, 1, ..., 9}
            
            // SAMPLE random timestep t ~ U(0, 1)
            N = x_1.shape[0]
            t = sample_uniform(N, min=0, max=1)  // t ~ U(0, 1)
            t_normalized = t.view(N, 1)  // Reshape for UNet input
            
            // SAMPLE noise
            ε = sample_gaussian_noise(shape=x_1.shape)  // ε ~ N(0, I)
            
            // CREATE intermediate noisy image via linear interpolation
            x_t = (1 - t.view(N, 1, 1, 1)) * ε + t.view(N, 1, 1, 1) * x_1
            
            // COMPUTE ground-truth flow
            v_true = x_1 - ε  // Flow from noise to clean image
            
            // CLASSIFIER-FREE GUIDANCE: randomly mask out class conditioning
            mask = (sample_uniform(N) > p_uncond).float()  // 90% keep (mask=1), 10% drop (mask=0)
            
            // PREDICT flow using class-conditioned UNet
            // UNet internally uses mask to zero out class conditioning when mask=0
            v_pred = class_unet(x_t, c, t_normalized, mask=mask)
            
            // COMPUTE L2 loss
            loss = criterion(v_pred, v_true)
            
            // BACKPROPAGATE and update
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            train_losses.append(loss.item())
        
        scheduler.step()  // Decay learning rate after each epoch
    
    RETURN class_unet, train_losses
    
    Key insight: By randomly dropping class conditioning during training (10% of the time),
                 the model learns both conditional and unconditional flows, enabling
                 classifier-free guidance during sampling for improved quality.</code></pre>
                </div>

                <h4>Part 2.6: Sampling from the Class-Conditioned UNet</h4>
                <p>
                    Now we sample with class-conditioning and use classifier-free guidance with $\gamma = 5.0$ to improve generation 
                    quality and ensure class fidelity.
                </p>

                <div class="code-container">
                    <div class="code-title">Class-conditioned flow matching sampling with CFG (pseudocode)</div>
                    <pre><code>FUNCTION sample_class_conditioned_flow_matching(class_unet, class_label, img_wh, 
                                                  num_ts=300, guidance_scale=5.0, seed=0):
    """
    Generate class-specific images using flow matching with classifier-free guidance.
    
    Input:
        class_unet: Trained ClassConditionalUNet model
        class_label: Target class c ∈ {0, 1, ..., 9}
        img_wh: (height, width) tuple for output image size
        num_ts: Number of timesteps for Euler integration (default 300)
        guidance_scale: CFG scale γ (default 5.0)
        seed: Random seed for reproducibility
    
    Output:
        generated_image: Generated image x₁ of specified class
    """
    
    set_random_seed(seed)
    device = get_device(class_unet)
    
    // START with pure noise
    x_t = sample_gaussian_noise(shape=(1, 1, img_wh[0], img_wh[1]), device=device)
    
    // CREATE timestep schedule: t from 0 to 1
    timesteps = linspace(0, 1, num_ts + 1)
    
    // PREPARE class conditioning
    c = tensor([class_label], device=device)  // Target class
    N = c.shape[0]
    
    // ITERATIVE denoising via Euler integration with CFG
    FOR i IN range(num_ts):
        t = timesteps[i]
        t_next = timesteps[i + 1]
        dt = t_next - t  // Step size for this iteration
        t_batch = t.view(1, 1).expand(N, 1)  // Expand timestep for batch
        
        // COMPUTE CONDITIONAL flow (with class conditioning enabled)
        mask_cond = tensor([1.0] * N, device=device)  // mask=1 keeps conditioning
        v_cond = class_unet(x_t, c, t_batch, mask=mask_cond)  // u_θ(x_t, t, c)
        
        // COMPUTE UNCONDITIONAL flow (class conditioning masked out)
        mask_uncond = tensor([0.0] * N, device=device)  // mask=0 drops conditioning
        v_uncond = class_unet(x_t, c, t_batch, mask=mask_uncond)  // u_θ(x_t, t, c with mask=0)
        
        // APPLY CLASSIFIER-FREE GUIDANCE
        // u = u_uncond + γ · (u_cond - u_uncond)
        v = v_uncond + guidance_scale * (v_cond - v_uncond)
        
        // UPDATE state using Euler integration
        x_t = x_t + dt * v
    
    RETURN x_t
    
    Key insight: CFG amplifies the difference between conditional and unconditional
                 flows, steering generation toward the target class while maintaining
                 high quality. Higher guidance_scale increases class fidelity but
                 may reduce diversity.</code></pre>
                </div>

                <div class="image-container">
                    <img src="project5/project5b_images/class_conditioned_train_loss.jpg" alt="Class-conditioned training loss">
                    <div class="project-caption">Training loss curve for the class-conditioned UNet.</div>
                </div>
                <div class="image-container">
                    <img src="project5/project5b_images/part25_class_conditioned_samples_Epoch 1.jpg" alt="Class-conditioned sampling grid epoch 1">
                    <div class="project-caption">Sampling results from the class-conditioned UNet after epoch 1 (4 samples per digit).</div>
                </div>
                <div class="image-container">
                    <img src="project5/project5b_images/part25_class_conditioned_samples_Epoch 5.jpg" alt="Class-conditioned sampling grid epoch 5">
                    <div class="project-caption">Sampling results from the class-conditioned UNet after epoch 5 (4 samples per digit).</div>
                </div>
                <div class="image-container">
                    <img src="project5/project5b_images/part25_class_conditioned_samples_Epoch 10.jpg" alt="Class-conditioned sampling grid epoch 10">
                    <div class="project-caption">Sampling results from the class-conditioned UNet after epoch 10 (4 samples per digit).</div>
                </div>

                <h4>No-Scheduler Ablation Study</h4>
                <p>
                    As requested, I conducted an ablation study to remove the exponential learning rate scheduler while maintaining 
                    comparable performance. I experimented with two approaches:
                </p>
                <ol>
                    <li><strong>Fixed learning rate with Adam:</strong> Removed the scheduler and used a lower learning rate 
                        of $10^{-4}$ throughout training.</li>
                    <li><strong>Lower learning rate with AdamW:</strong> Used AdamW optimizer (which includes weight decay) with 
                        a lower fixed learning rate of $5 \times 10^{-4}$ and weight decay of $10^{-4}$.</li>
                </ol>
                <p>
                    Both approaches achieved comparable performance to the scheduler-based training. The key insight is that 
                    using a lower fixed learning rate (compared to the initial scheduler LR of $10^{-2}$) prevents training 
                    instability, while weight decay in AdamW provides additional regularization that helps stabilize training 
                    without the need for learning rate decay.
                </p>
                <div class="image-container">
                    <img src="project5/project5b_images/class_conditioned_no_scheduler_train_loss.jpg" alt="Class-conditioned no scheduler training loss">
                    <div class="project-caption">Training loss for class-conditioned UNet without scheduler, using Adam with fixed LR $10^{-4}$.</div>
                </div>
                <div class="image-container">
                    <img src="project5/project5b_images/part25_class_conditioned_no_scheduler_samples_Epoch 1.jpg" alt="Class-conditioned no scheduler samples epoch 1">
                    <div class="project-caption">Sampling results from the class-conditioned UNet trained without scheduler (Adam, fixed LR $10^{-4}$) after epoch 1.</div>
                </div>
                <div class="image-container">
                    <img src="project5/project5b_images/part25_class_conditioned_no_scheduler_samples_Epoch 5.jpg" alt="Class-conditioned no scheduler samples epoch 5">
                    <div class="project-caption">Sampling results from the class-conditioned UNet trained without scheduler (Adam, fixed LR $10^{-4}$) after epoch 5.</div>
                </div>
                <div class="image-container">
                    <img src="project5/project5b_images/part25_class_conditioned_no_scheduler_samples_Epoch 10.jpg" alt="Class-conditioned no scheduler samples epoch 10">
                    <div class="project-caption">Sampling results from the class-conditioned UNet trained without scheduler (Adam, fixed LR $10^{-4}$) after epoch 10.</div>
                </div>
                <div class="image-container">
                    <img src="project5/project5b_images/class_conditioned_adamw_no_scheduler_train_loss.jpg" alt="Class-conditioned AdamW no scheduler training loss">
                    <div class="project-caption">Training loss for class-conditioned UNet without scheduler, using AdamW with fixed LR $5 \times 10^{-4}$ and weight decay $10^{-4}$.</div>
                </div>
                <div class="image-container">
                    <img src="project5/project5b_images/part25_class_conditioned_adamw_no_scheduler_samples_Epoch 1.jpg" alt="Class-conditioned AdamW no scheduler samples epoch 1">
                    <div class="project-caption">Sampling results from the class-conditioned UNet trained without scheduler (AdamW, fixed LR $5 \times 10^{-4}$, weight decay $10^{-4}$) after epoch 1.</div>
                </div>
                <div class="image-container">
                    <img src="project5/project5b_images/part25_class_conditioned_adamw_no_scheduler_samples_Epoch 5.jpg" alt="Class-conditioned AdamW no scheduler samples epoch 5">
                    <div class="project-caption">Sampling results from the class-conditioned UNet trained without scheduler (AdamW, fixed LR $5 \times 10^{-4}$, weight decay $10^{-4}$) after epoch 5.</div>
                </div>
                <div class="image-container">
                    <img src="project5/project5b_images/part25_class_conditioned_adamw_no_scheduler_samples_Epoch 10.jpg" alt="Class-conditioned AdamW no scheduler samples epoch 10">
                    <div class="project-caption">Sampling results from the class-conditioned UNet trained without scheduler (AdamW, fixed LR $5 \times 10^{-4}$, weight decay $10^{-4}$) after epoch 10. This configuration achieves comparable quality to the scheduler-based training.</div>
                </div>
                <p>
                    <strong>Results and analysis:</strong> The no-scheduler variants achieved comparable generation quality to 
                    the scheduler-based training, with some differences between approaches. The Adam variant with fixed LR $10^{-4}$ 
                    performed very well, producing high-quality digits with only minor issues (one generated "2" digit showing some 
                    artifacts). The AdamW variant with fixed LR $5 \times 10^{-4}$ and weight decay $10^{-4}$ also achieved good 
                    results, though it exhibited slightly more artifacting in some generated samples compared to the Adam variant. 
                    This demonstrates that we can simplify the training setup by removing the scheduler while maintaining performance 
                    through careful hyperparameter tuning—specifically, using a lower fixed learning rate (compared to the initial 
                    scheduler LR of $10^{-2}$) prevents training instability. The Adam variant with a simple fixed LR appears to be 
                    the most straightforward solution, achieving excellent results without the complexity of weight decay.
                </p>
            </div>
        </div>
    </main>
    <script src="../ascii-loader.js"></script>
</body>
</html>

