<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project 5: The Power of Diffusion Models - Part A</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@200..900&family=Newsreader:ital,opsz,wght@0,6..72,200..800;1,6..72,200..800&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../styles.css">
    
    <!-- MathJax for mathematical equations -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']]
            }
        };
    </script>
</head>
<body>
    <header>
        <a href="../index.html" style="text-decoration: none;"><h1 class="site-title">Elizabeth Weaver</h1></a>
        <nav>
            <a href="../index.html" class="nav-gray">Home</a>
            <a href="../projects.html" class="nav-dark">Projects</a>
            <a href="../blog.html" class="nav-green">Blog</a>
        </nav>
    </header>

    <div class="ascii-art"></div>

    <div class="breadcrumb">
        <a href="../projects.html">Projects</a> / <a href="index.html">CS180</a> / Project 5: The Power of Diffusion Models - Part A
    </div>

    <main class="main-content projects with-breadcrumb">
        <div class="profile-section">
            <div class="project-header">
                <h1>Part A: The Power of Diffusion Models!</h1>
                <p>
                    In this project, I explore diffusion models using the DeepFloyd IF model. I implement diffusion 
                    sampling loops from scratch, experiment with denoising techniques, and use diffusion models for 
                    creative tasks such as inpainting, image-to-image translation, visual anagrams, and hybrid images. 
                    This project demonstrates the power of modern generative AI models and their applications in 
                    computational photography.
                </p>
            </div>

            <div class="project-section">
                <h2>Part 0: Setup</h2>
                <p>
                    I used the DeepFloyd IF diffusion model, a two-stage model trained by Stability AI. The first stage 
                    produces images of size 64×64, and the second stage upsamples them to 256×256. I accessed the model 
                    through Hugging Face after accepting the usage conditions and logging in with my access token.
                </p>
                <p>
                    To generate images from text prompts, I needed to convert text strings into high-dimensional prompt 
                    embeddings (4096 dimensions). I used Hugging Face clusters to generate embeddings for my custom 
                    text prompts, which I then loaded into the notebook.
                </p>
                <p>
                    I created embeddings for the following prompts:
                </p>
                <ul>
                    <li>"a high quality photo" (used as a baseline/null condition)</li>
                    <li>"a watercolor painting of a wildflower meadow"</li>
                    <li>"a photo of a cat sleeping on a windowsill"</li>
                    <li>"an oil painting of a bustling farmer's market"</li>
                    <li>"a lithograph of a lighthouse on a cliff"</li>
                    <li>"a photo of a vintage bicycle leaning against a brick wall"</li>
                    <li>"a watercolor painting of a hot air balloon"</li>
                    <li>"a sketch of a piano player performing"</li>
                    <li>"a photo of frost on winter branches"</li>
                    <li>"a photo of a cozy library with warm lighting"</li>
                    <li>"a photo of a coffee cup with latte art"</li>
                    <li>"an oil painting of a stormy sea"</li>
                    <li>"a photo of dewdrops on a spider web"</li>
                    <li>"" (empty prompt for unconditional generation)</li>
                </ul>
                <p>
                    I used a consistent random seed throughout all experiments to ensure reproducibility. The random seed 
                    value I used was <strong>42</strong>, which I set at the beginning of the notebook and used consistently 
                    for all subsequent parts.
                </p>
                <p>
                    I chose three of my prompts to generate images and explore how different settings affect the output quality:
                </p>
                <ol>
                    <li><strong>"a watercolor painting of a wildflower meadow"</strong> - An artistic watercolor style prompt</li>
                    <li><strong>"a photo of a cat sleeping on a windowsill"</strong> - A photorealistic subject prompt</li>
                    <li><strong>"an oil painting of a bustling farmer's market"</strong> - An artistic oil painting style prompt</li>
                </ol>
                <p>
                    I experimented with different <code>num_inference_steps</code> values to understand the trade-off 
                    between generation speed and image quality. The <code>num_inference_steps</code> parameter controls how 
                    many denoising steps the model takes: fewer steps are faster but produce lower quality images, while 
                    more steps take longer but generate higher quality results.
                </p>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/part0_numinference_out.jpg" alt="Comparison of different num_inference_steps">
                    <div class="project-caption">Comparison of different num_inference_steps: Generated images for three chosen prompts at different num_inference_steps values (2, 20, and 200). The prompts are: "a watercolor painting of a wildflower meadow" (left column), "a photo of a cat sleeping on a windowsill" (middle column), and "an oil painting of a bustling farmer's market" (right column). The images clearly demonstrate how increasing num_inference_steps dramatically improves image quality—from pure noise at 2 steps, to recognizable but pixelated images at 20 steps, to clear, detailed images at 200 steps.</div>
                </div>
                <p>
                    <strong>Reflections on output quality:</strong> The diffusion model demonstrates impressive capability 
                    in generating images that match text prompts. The quality of outputs varies based on the prompt type—photorealistic 
                    prompts like "a photo of a cat sleeping on a windowsill" produce detailed, realistic images, while artistic 
                    prompts like "a watercolor painting of a wildflower meadow" and "an oil painting of a bustling farmer's market" 
                    generate stylized images that match the artistic medium described. The relationship between prompts and outputs 
                    is strong, with the model successfully interpreting both concrete subjects (cat, market scenes) and artistic 
                    styles (watercolor, oil painting). 
                </p>
                <p>
                    The comparison across different <code>num_inference_steps</code> values reveals a clear progression: at 2 steps, 
                    the images are essentially pure noise with no recognizable content. At 20 steps, basic structure and color 
                    coherence emerge, but images remain heavily pixelated and abstract. At 200 steps, the images become clear, 
                    detailed, and closely match their text descriptions. 
                </p>
                <p>
                    The improvement varies by prompt type. For photorealistic prompts like "a photo of a cat sleeping on a windowsill", 
                    both 20 and 200 steps produce photorealistic results, though 200 steps shows finer detail. For more complex 
                    artistic scenes like "an oil painting of a bustling farmer's market", the difference is more pronounced: at 20 
                    steps the scene is recognizable but lacks the dynamic "bustling" quality, while at 200 steps the model better 
                    captures the energy and activity suggested by the prompt. This demonstrates that while fewer steps are faster, 
                    significantly more steps are required to achieve nuanced interpretations of complex prompts. The improvement is 
                    most dramatic between 2 and 20 steps, with continued refinement up to 200 steps, especially for prompts requiring 
                    complex scene understanding.
                </p>
            </div>

            <div class="project-section">
                <h2>Part 1: Sampling Loops</h2>
                <p>
                    In this section, I implement the core components of diffusion models: the forward process (adding 
                    noise), denoising operations, and iterative sampling loops. These form the foundation for all 
                    subsequent applications.
                </p>
            </div>

            <div class="project-section">
                <h2>Part 1.1: Implementing the Forward Process</h2>
                <p>
                    The forward process is fundamental to diffusion models. It takes a clean image and adds noise to it 
                    according to a predefined schedule. The forward process is defined by:
                </p>
                $$q(x_t | x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t) I)$$
                <p>
                    where $\bar{\alpha}_t$ (alpha_cumprod) controls the amount of noise at timestep $t$. When $t=0$, we 
                    have a clean image, and for larger $t$, more noise is added. The $\bar{\alpha}_t$ values are close 
                    to 1 for small $t$ and close to 0 for large $t$.
                </p>
                <p>
                    I implemented the <code>forward(im, t)</code> function that takes a clean image and a timestep, 
                    then samples a noisy version according to the diffusion schedule. I tested this on a 64×64 resized 
                    image of the Berkeley Campanile at noise levels 250, 500, and 750.
                </p>

                <div class="code-container">
                    <div class="code-title">Forward process implementation (pseudocode)</div>
                    <pre><code>FUNCTION forward(clean_image, timestep):
    """
    Add noise to a clean image according to the diffusion schedule.
    
    Input:
        clean_image: Clean image x₀
        timestep: Integer t from 0 to 1000
    
    Output:
        noisy_image: Noisy image x_t at timestep t
    """
    
    1. GET noise schedule parameter:
       - ᾱ_t = lookup_cumulative_alpha(timestep)
       - This is a precomputed value controlling signal vs noise
    
    2. COMPUTE scaling factors:
       - signal_scale = √(ᾱ_t)
       - noise_scale = √(1 - ᾱ_t)
    
    3. SAMPLE random noise:
       - ε ~ N(0, I)  // Sample Gaussian noise with same shape as image
    
    4. APPLY forward diffusion formula:
       - noisy_image = signal_scale · clean_image + noise_scale · ε
       - This implements: x_t = √(ᾱ_t) · x₀ + √(1 - ᾱ_t) · ε
    
    5. RETURN noisy_image
    
    Key insight: At t=0, ᾱ_t ≈ 1, so noisy_image ≈ clean_image
                 At t=1000, ᾱ_t ≈ 0, so noisy_image ≈ pure noise</code></pre>
                </div>

                <div class="image-container">
                    <img src="project5/proj5a_imgs/campanile_noise_levels.jpg" alt="Campanile at different noise levels">
                    <div class="project-caption">Campanile at different noise levels: The forward process applied to the Campanile at timesteps t=250, t=500, and t=750. As the timestep increases, more noise is added to the image, progressively obscuring the original structure.</div>
                </div>
            </div>

            <div class="project-section">
                <h2>Part 1.2: Classical Denoising</h2>
                <p>
                    Before using the diffusion model, I attempted to denoise the images using classical methods—specifically, 
                    Gaussian blur filtering. This serves as a baseline to demonstrate why diffusion models are necessary. 
                    I applied Gaussian blur to the noisy Campanile images at timesteps 250, 500, and 750.
                </p>
                <p>
                    As expected, Gaussian blur filtering struggles to remove noise while preserving image details. The 
                    results show that simple filtering cannot effectively separate signal from noise, especially at high 
                    noise levels. This motivates the need for learned denoising models.
                </p>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/step250_gauss_blur.jpg" alt="Gaussian blur denoising at t=250">
                    <div class="project-caption">Gaussian blur denoising at t=250 using kernel size 7, 11, 15. The blur reduces noise but also removes important image details. Kernel size 15 almost removes the noise.</div>
                </div>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/step500_gauss_blur.jpg" alt="Gaussian blur denoising at t=500">
                    <div class="project-caption">Gaussian blur denoising at t=500 using kernel size 15, 19, 23. At higher noise levels, Gaussian blur becomes increasingly ineffective. Kernel size 23 almost removes the noise, but the campanile's windows are completely lost.</div>
                </div>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/step750_gauss_blur.jpg" alt="Gaussian blur denoising at t=750">
                    <div class="project-caption">Gaussian blur denoising at t=750 using kernel size 23, 27, 31. The classical method fails to recover meaningful structure from heavily noisy images. Kernel size 31 removes some noise from the sky, but at this point the campanile is extremely hard to make out.</div>
                </div>
            </div>

            <div class="project-section">
                <h2>Part 1.3: One-Step Denoising</h2>
                <p>
                    Now I use the pretrained diffusion model UNet to denoise images. The UNet takes a noisy image and 
                    timestep as input and predicts the noise in the image. With the predicted noise, I can estimate the 
                    clean image.
                </p>
                <p>
                    The key insight is that the UNet was trained on millions of (noisy image, timestep, clean image) 
                    pairs, so it has learned to project noisy images back onto the manifold of natural images. However, 
                    for a single denoising step, the quality degrades as more noise is added, since the problem becomes 
                    increasingly difficult.
                </p>
                <p>
                    I implemented one-step denoising for the three noisy Campanile images (t = 250, 500, 750). The results 
                    show that the diffusion model performs significantly better than Gaussian blur, especially at moderate 
                    noise levels. However, at very high noise levels (t=750), even the diffusion model struggles with 
                    a single step.
                </p>
                <p>
                    For each of the three noisy images (t = [250, 500, 750]), I show the original image, the noisy image, 
                    and the one-step denoised estimate side by side:
                </p>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/denoising_250.png" alt="One-step denoising at t=250">
                    <div class="project-caption">One-step denoising at t=250: Original image (left), noisy image (middle), and denoised estimate (right). The diffusion model successfully recovers most of the structure at this moderate noise level.</div>
                </div>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/denoising_500.png" alt="One-step denoising at t=500">
                    <div class="project-caption">One-step denoising at t=500: Original image (left), noisy image (middle), and denoised estimate (right). At this higher noise level, the estimate shows more artifacts but still captures the main structure.</div>
                </div>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/denoising_750.png" alt="One-step denoising at t=750">
                    <div class="project-caption">One-step denoising at t=750: Original image (left), noisy image (middle), and denoised estimate (right). At this very high noise level, the structure of the campanile is starting to be lost, the top now having more of a dome-like shape.</div>
                </div>
            </div>

            <div class="project-section">
                <h2>Part 1.4: Iterative Denoising</h2>
                <p>
                    Diffusion models are designed to denoise iteratively. Instead of removing all noise in one step, 
                    we gradually reduce noise over multiple steps. This is much more effective than single-step denoising.
                </p>
                <p>
                    To speed up the process, I use strided timesteps—starting at timestep 990 and taking steps of size 
                    30 until reaching 0. This creates a schedule: [990, 960, 930, ..., 30, 0]. The rationale for step 
                    skipping comes from connections with differential equations, allowing us to take larger steps while 
                    maintaining quality.
                </p>
                <p>
                    At each step, I compute the next less-noisy image using the formula:
                </p>
                $$x_{t'} = \sqrt{\bar{\alpha}_{t'}} \hat{x}_0 + \sqrt{1 - \bar{\alpha}_{t'}} \epsilon$$
                <p>
                    where $\hat{x}_0$ is the estimated clean image from one-step denoising, and $\epsilon$ includes 
                    predicted variance for randomness.
                </p>

                <div class="code-container">
                    <div class="code-title">Strided timesteps and iterative denoising (pseudocode)</div>
                    <pre><code># STEP 1: Create strided timesteps
FUNCTION create_strided_timesteps():
    """
    Create a list of timesteps with stride 30, starting at 990, ending at 0.
    This speeds up sampling by skipping intermediate steps.
    """
    
    timesteps = []
    current = 990
    
    WHILE current >= 0:
        timesteps.append(current)
        current = current - 30
    
    // Result: [990, 960, 930, ..., 30, 0]
    
    RETURN timesteps


# STEP 2: Iterative denoising function
FUNCTION iterative_denoise(noisy_image, start_index, prompt_embeddings, timesteps):
    """
    Denoise an image iteratively from timesteps[start_index] to timesteps[0].
    
    Input:
        noisy_image: Noisy image x_t
        start_index: Starting index in timesteps array
        prompt_embeddings: Text prompt embeddings for conditioning
        timesteps: List of timesteps [990, 960, ..., 0]
    
    Output:
        clean_image: Denoised image x₀
        intermediate_images: List of images at each step
    """
    
    current_image = noisy_image
    intermediate_images = []
    
    FOR i FROM start_index TO length(timesteps) - 2:
        
        // Get current and next timesteps
        t = timesteps[i]        // Current timestep (more noisy)
        t' = timesteps[i+1]     // Next timestep (less noisy)
        
        // Get noise schedule parameters
        ᾱ_t = lookup_cumulative_alpha(t)
        ᾱ_t' = lookup_cumulative_alpha(t')
        α_t = ᾱ_t / ᾱ_t'
        β_t = 1 - α_t
        
        // PREDICT NOISE using diffusion model
        model_output = diffusion_model(current_image, t, prompt_embeddings)
        // Model outputs [noise_estimate, variance_estimate]
        noise_estimate = extract_noise_estimate(model_output)
        variance_estimate = extract_variance_estimate(model_output)
        
        // ESTIMATE CLEAN IMAGE from noisy image and predicted noise
        x₀_estimate = (current_image - √(1 - ᾱ_t) · noise_estimate) / √(ᾱ_t)
        
        // COMPUTE COEFFICIENTS for interpolation
        x₀_coeff = √(ᾱ_t') · β_t / (1 - ᾱ_t)
        x_t_coeff = √(α_t) · (1 - ᾱ_t') / (1 - ᾱ_t)
        
        // INTERPOLATE between estimated clean image and current noisy image
        next_image = x₀_coeff · x₀_estimate + x_t_coeff · current_image
        
        // ADD VARIANCE for randomness (prevents mode collapse)
        next_image = add_variance(variance_estimate, t, next_image)
        
        // UPDATE for next iteration
        current_image = next_image
        intermediate_images.append(current_image)
    
    RETURN current_image, intermediate_images</code></pre>
                </div>

                <p>
                    I tested iterative denoising on the Campanile image starting from timestep 90 (i_start=10). The 
                    results show progressive improvement as noise is gradually removed.
                </p>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/iterative_denoise_5_loop.jpg" alt="Iterative denoising progression">
                    <div class="project-caption">Iterative denoising progression showing every 5th step, starting from timestep 90 (i_start=10). The image gradually becomes cleaner as noise is removed iteratively. The final result is much better than one-step denoising or Gaussian blur. From left to right, the timesteps are 690, 540, 390, 240, and 90.</div>
                </div>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/iterative_vs_onestep.jpg" alt="Comparison of denoising methods">
                    <div class="project-caption">Comparison of different denoising methods: original noisy image, iteratively denoised result, one-step denoising (much worse quality), and Gaussian blur filtering (kernel size 5, sigma=2). This demonstrates that iterative denoising produces significantly better results than single-step denoising or classical filtering methods.</div>
                </div>
            </div>

            <div class="project-section">
                <h2>Part 1.5: Diffusion Model Sampling</h2>
                <p>
                    One of the most powerful applications of diffusion models is generating images from scratch. By 
                    starting with pure random noise and iteratively denoising it, we can create entirely new images.
                </p>
                <p>
                    I implemented image generation by setting i_start=0 (starting from pure noise at timestep 990) and 
                    running the iterative denoising process. I generated 5 images using the prompt "a high quality photo" 
                    as a weak condition.
                </p>
                <p>
                    The results show that the model can generate diverse, coherent images from random noise. However, 
                    without stronger conditioning, the images are somewhat generic and lack specific content. This 
                    motivates the use of Classifier-Free Guidance (CFG) in the next section.
                </p>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/sampled_images.jpg" alt="Sampled images without CFG">
                    <div class="project-caption">Sampled images without CFG: Five images sampled from pure noise using the prompt "a high quality photo". The images are diverse but somewhat generic without stronger text conditioning.</div>
                </div>
            </div>

            <div class="project-section">
                <h2>Part 1.6: Classifier-Free Guidance (CFG)</h2>
                <p>
                    Classifier-Free Guidance dramatically improves image quality by using both conditional and 
                    unconditional noise estimates. The formula is:
                </p>
                $$\epsilon = \epsilon_{\text{uncond}} + \gamma (\epsilon_{\text{cond}} - \epsilon_{\text{uncond}})$$
                <p>
                    where $\gamma$ (the CFG scale) controls the strength of guidance. For $\gamma=1$, we get the 
                    conditional estimate. For $\gamma > 1$, we amplify the difference between conditional and 
                    unconditional estimates, leading to higher quality but less diverse images.
                </p>
                <p>
                    I implemented <code>iterative_denoise_cfg</code> which computes both conditional and unconditional 
                    noise estimates at each step. The unconditional estimate uses an empty prompt embedding, while the 
                    conditional uses the text prompt embedding.
                </p>

                <div class="code-container">
                    <div class="code-title">Classifier-Free Guidance implementation (pseudocode)</div>
                    <pre><code>FUNCTION iterative_denoise_cfg(noisy_image, start_index, conditional_prompt, 
                                unconditional_prompt, timesteps, cfg_scale=7):
    """
    Iterative denoising with Classifier-Free Guidance (CFG).
    CFG improves quality by amplifying the difference between conditional 
    and unconditional noise estimates.
    
    Input:
        noisy_image: Noisy image x_t
        start_index: Starting index in timesteps array
        conditional_prompt: Text prompt embeddings (with text)
        unconditional_prompt: Empty prompt embeddings (unconditional)
        timesteps: List of timesteps
        cfg_scale: CFG scale γ, typically 7.0
    
    Output:
        clean_image: Denoised image with CFG
        intermediate_images: List of images at each step
    """
    
    current_image = noisy_image
    intermediate_images = []
    
    FOR i FROM start_index TO length(timesteps) - 2:
        
        t = timesteps[i]
        t' = timesteps[i+1]
        ᾱ_t = lookup_cumulative_alpha(t)
        ᾱ_t' = lookup_cumulative_alpha(t')
        α_t = ᾱ_t / ᾱ_t'
        β_t = 1 - α_t
        
        // COMPUTE CONDITIONAL noise estimate (with text prompt)
        cond_output = diffusion_model(current_image, t, conditional_prompt)
        ε_cond = extract_noise_estimate(cond_output)
        variance = extract_variance_estimate(cond_output)
        
        // COMPUTE UNCONDITIONAL noise estimate (empty prompt)
        uncond_output = diffusion_model(current_image, t, unconditional_prompt)
        ε_uncond = extract_noise_estimate(uncond_output)
        
        // APPLY CFG FORMULA: amplify difference between conditional and unconditional
        ε = ε_uncond + cfg_scale · (ε_cond - ε_uncond)
        // For cfg_scale=7: ε = ε_uncond + 7 · (ε_cond - ε_uncond)
        // This amplifies the effect of the text prompt
        
        // ESTIMATE CLEAN IMAGE using CFG noise estimate
        x₀_estimate = (current_image - √(1 - ᾱ_t) · ε) / √(ᾱ_t)
        
        // COMPUTE INTERPOLATION COEFFICIENTS
        x₀_coeff = √(ᾱ_t') · β_t / (1 - ᾱ_t)
        x_t_coeff = √(α_t) · (1 - ᾱ_t') / (1 - ᾱ_t)
        
        // INTERPOLATE to get next less-noisy image
        next_image = x₀_coeff · x₀_estimate + x_t_coeff · current_image
        
        // ADD VARIANCE (use conditional variance)
        next_image = add_variance(variance, t, next_image)
        
        current_image = next_image
        intermediate_images.append(current_image)
    
    RETURN current_image, intermediate_images</code></pre>
                </div>

                <p>
                    I generated 5 images with CFG scale $\gamma=7$ using the prompt "a high quality photo". The results 
                    show significantly improved quality compared to sampling without CFG.
                </p>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/sampled_images_cfg.jpg" alt="Sampled images with CFG">
                    <div class="project-caption">Sampled images with CFG: Five images sampled with CFG scale γ=7. The images show improved quality and coherence compared to sampling without CFG.</div>
                </div>
            </div>

            <div class="project-section">
                <h2>Part 1.7: Image-to-image Translation</h2>
                <p>
                    Diffusion models can be used to edit existing images through a technique called SDEdit. The idea is 
                    to add a controlled amount of noise to an image, then denoise it. The denoising process "forces" the 
                    noisy image back onto the natural image manifold, effectively making edits.
                </p>
                <p>
                    The amount of noise added controls the strength of the edit: more noise leads to larger changes, 
                    while less noise preserves more of the original. This works because the diffusion model must 
                    "hallucinate" new content to denoise the image, and this creativity enables editing.
                </p>
                <p>
                    I tested SDEdit on the Campanile image using starting indices [1, 3, 5, 7, 10, 20] (corresponding to 
                    different starting points in the strided timesteps). The parameter <code>i_start</code> controls how much 
                    noise is added: lower values (like i_start=1) mean starting from a noisier state, resulting in more 
                    dramatic transformations that look less like the original. Higher values (like i_start=20) mean starting 
                    from a less noisy state, resulting in images that look more similar to the original. The results show 
                    a gradual transition from heavily edited versions (left) to images that closely match the original (right).
                </p>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/campanile_img_to_img.jpg" alt="Campanile SDEdit at different noise levels">
                    <div class="project-caption">Campanile SDEdit at different starting indices [1, 3, 5, 7, 10, 20] (left to right), with the original image on the rightmost side. Lower i_start values (left) correspond to more noise added, creating more dramatic edits that look less like the original. Higher i_start values (right) correspond to less noise, preserving more of the original appearance.</div>
                </div>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/loki_img_to_img.jpg" alt="Loki SDEdit at different noise levels">
                    <div class="project-caption">Loki SDEdit at different starting indices [1, 3, 5, 7, 10, 20] (left to right), with the original image on the rightmost side. Lower i_start values (left) add more noise, resulting in more transformed images. Higher i_start values (right) add less noise, resulting in images that look more similar to the original.</div>
                </div>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/windmill_img_to_img.jpg" alt="Windmill SDEdit at different noise levels">
                    <div class="project-caption">Windmill SDEdit at different starting indices [1, 3, 5, 7, 10, 20] (left to right), with the original image on the rightmost side. The transformation strength decreases from left to right: i_start=1 produces the most dramatic edit (most noise), while i_start=20 produces the most conservative edit (least noise, closest to original).</div>
                </div>
            </div>

            <div class="project-section">
                <h2>Part 1.7.1: Editing Hand-Drawn and Web Images</h2>
                <p>
                    SDEdit works particularly well when starting with non-realistic images (paintings, sketches, drawings) 
                    and projecting them onto the natural image manifold. This allows us to "realize" hand-drawn sketches 
                    or stylized images as photorealistic photos.
                </p>
                <p>
                    I tested this on:
                </p>
                <ol>
                    <li><strong>Web image:</strong> A drawing tutorial image downloaded from the internet</li>
                    <li><strong>Hand-drawn images:</strong> Two sketches I drew using the provided drawing interface</li>
                </ol>
                <p>
                    For each image, I applied SDEdit at starting indices [1, 3, 5, 7, 10, 20] to show the progression from 
                    the original drawing to a more realistic image. Lower i_start values add more noise, creating more 
                    dramatic transformations, while higher values preserve more of the original drawing.
                </p>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/web_img_to_img_all_i_start.jpg" alt="Web image SDEdit">
                    <div class="project-caption">Web image SDEdit: Applied to a web image (drawing tutorial) at different starting indices [1, 3, 5, 7, 10, 20] (left to right), with the original image on the rightmost side. Lower i_start values (left) add more noise, creating more dramatic transformations from stylized drawing to realistic image. Higher i_start values (right) add less noise, preserving more of the original drawing style.</div>
                </div>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/dog_drawing_img_to_img_all_i_start.jpg" alt="Hand-drawn dog SDEdit">
                    <div class="project-caption">Hand-drawn dog SDEdit: Applied to a hand-drawn dog sketch at different starting indices [1, 3, 5, 7, 10, 20] (left to right), with the original image on the rightmost side. Lower i_start values (left) add more noise, resulting in more realistic interpretations. Higher i_start values (right) add less noise, keeping the image closer to the original sketch while still adding realistic details.</div>
                </div>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/mountain_drawing_img_to_img_all_i_start.jpg" alt="Hand-drawn mountain SDEdit">
                    <div class="project-caption">Hand-drawn mountain SDEdit: Applied to a hand-drawn mountain landscape at different starting indices [1, 3, 5, 7, 10, 20] (left to right), with the original image on the rightmost side. Lower i_start values (left) add more noise, creating more dramatic transformations with realistic texture, lighting, and depth. Higher i_start values (right) add less noise, maintaining more of the original drawing structure.</div>
                </div>
            </div>

            <div class="project-section">
                <h2>Part 1.7.2: Inpainting</h2>
                <p>
                    Inpainting is the task of filling in missing or masked regions of an image. I implemented inpainting 
                    using the RePaint algorithm, which runs the diffusion denoising loop but at each step, "forces" the 
                    unmasked regions to match the original image (with appropriate noise added for the current timestep).
                </p>
                <p>
                    The algorithm works by:
                </p>
                <ol>
                    <li>Running the normal denoising loop to get $x_{t'}$</li>
                    <li>Replacing pixels in the unmasked region with the original image (with noise added for timestep $t'$)</li>
                    <li>Repeating until we reach a clean image</li>
                </ol>

                <div class="code-container">
                    <div class="code-title">Inpainting implementation (pseudocode)</div>
                    <pre><code>FUNCTION inpaint(original_image, mask, conditional_prompt, unconditional_prompt, 
                 timesteps, cfg_scale=7):
    """
    Inpaint masked regions of an image using the RePaint algorithm.
    
    Input:
        original_image: Original clean image x₀
        mask: Binary mask (1 = region to inpaint, 0 = keep original)
        conditional_prompt: Text prompt embeddings
        unconditional_prompt: Unconditional embeddings
        timesteps: List of timesteps
        cfg_scale: CFG scale
    
    Output:
        inpainted_image: Image with masked regions filled in
    """
    
    // Start with pure noise
    current_image = sample_gaussian_noise(shape_of(original_image))
    
    FOR i FROM 0 TO length(timesteps) - 2:
        
        t = timesteps[i]
        t' = timesteps[i+1]
        ᾱ_t = lookup_cumulative_alpha(t)
        ᾱ_t' = lookup_cumulative_alpha(t')
        α_t = ᾱ_t / ᾱ_t'
        β_t = 1 - α_t
        
        // DENOISE using CFG (same as iterative_denoise_cfg)
        cond_output = diffusion_model(current_image, t, conditional_prompt)
        uncond_output = diffusion_model(current_image, t, unconditional_prompt)
        
        ε_cond = extract_noise_estimate(cond_output)
        ε_uncond = extract_noise_estimate(uncond_output)
        variance = extract_variance_estimate(cond_output)
        
        ε = ε_uncond + cfg_scale · (ε_cond - ε_uncond)
        
        x₀_estimate = (current_image - √(1 - ᾱ_t) · ε) / √(ᾱ_t)
        x₀_coeff = √(ᾱ_t') · β_t / (1 - ᾱ_t)
        x_t_coeff = √(α_t) · (1 - ᾱ_t') / (1 - ᾱ_t)
        
        next_image = x₀_coeff · x₀_estimate + x_t_coeff · current_image
        next_image = add_variance(variance, t, next_image)
        
        // KEY STEP: FORCE unmasked regions to match original (with noise)
        // Add appropriate noise to original image for timestep t'
        original_with_noise = forward(original_image, t')
        
        // Replace unmasked pixels (where mask == 0) with original_with_noise
        // Keep inpainted pixels (where mask == 1) from denoising
        next_image = mask · next_image + (1 - mask) · original_with_noise
        
        current_image = next_image
    
    RETURN current_image</code></pre>
                </div>

                <p>
                    I tested inpainting on three images:
                </p>
                <ol>
                    <li><strong>Campanile:</strong> Inpainted the top of the tower</li>
                    <li><strong>Window:</strong> Inpainted a rectangular region in the center</li>
                    <li><strong>Person:</strong> Inpainted an elliptical region (face removal)</li>
                </ol>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/inpainted_campanile.jpg" alt="Inpainted Campanile">
                    <div class="project-caption">Inpainted Campanile: The top of the tower was masked and filled in by the diffusion model, creating a plausible completion.</div>
                </div>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/inpainted_window.jpg" alt="Inpainted window">
                    <div class="project-caption">Inpainted window: The rectangular masked region was filled with content that matches the surrounding context.</div>
                </div>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/inpainted_person.jpg" alt="Inpainted person">
                    <div class="project-caption">Inpainted person: The elliptical mask (face region) was filled in, demonstrating the model's ability to generate plausible content in masked areas.</div>
                </div>
            </div>

            <div class="project-section">
                <h2>Part 1.7.3: Text-Conditioned Image-to-image Translation</h2>
                <p>
                    By using specific text prompts instead of the generic "a high quality photo", we can guide the 
                    image-to-image translation process. This combines SDEdit with text conditioning, allowing us to 
                    transform images according to both the original content and the text description.
                </p>
                <p>
                    I tested text-conditioned SDEdit on:
                </p>
                <ol>
                    <li><strong>Campanile:</strong> Transformed using prompts like "a lithograph of a lighthouse on a cliff" and "an oil painting of a stormy sea"</li>
                    <li><strong>Two additional images:</strong> Applied the same technique with various artistic style prompts</li>
                </ol>
                <p>
                    The results show that the images gradually look more like the original (as i_start increases and less 
                    noise is added) but also incorporate elements suggested by the text prompt, creating interesting artistic 
                    transformations. Lower i_start values add more noise, creating more dramatic style transformations, while 
                    higher i_start values add less noise, preserving more of the original image structure.
                </p>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/text_conditioned_campanile_denoising.jpg" alt="Text-conditioned Campanile">
                    <div class="project-caption">Text-conditioned Campanile: SDEdit using the prompt "a lithograph of a lighthouse on a cliff" at starting indices [1, 3, 5, 7, 10, 20] (left to right), with the original image on the rightmost side. Lower i_start values (left) add more noise, creating more dramatic transformations to match the artistic style. Higher i_start values (right) add less noise, preserving more of the original Campanile image.</div>
                </div>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/sea_text_conditioned_campanile_denoising.jpg" alt="Stormy sea Campanile">
                    <div class="project-caption">Stormy sea Campanile: Text-conditioned SDEdit using the prompt "an oil painting of a stormy sea" at starting indices [1, 3, 5, 7, 10, 20] (left to right), with the original image on the rightmost side. Lower i_start values (left) add more noise, creating more dramatic, painterly transformations. Higher i_start values (right) add less noise, resulting in subtler style changes that preserve more of the original image.</div>
                </div>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/text_conditioned_person_denoising.jpg" alt="Text-conditioned person">
                    <div class="project-caption">Text-conditioned person: SDEdit using the prompt "a photo of a coffee cup with latte art" at starting indices [1, 3, 5, 7, 10, 20] (left to right), with the original image on the rightmost side. Lower i_start values (left) add more noise, creating more dramatic style transformations. Higher i_start values (right) add less noise, resulting in subtler style changes that preserve more of the original image.</div>
                </div>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/text_conditioned_rose_denoising.jpg" alt="Text-conditioned rose">
                    <div class="project-caption">Text-conditioned rose: SDEdit using the prompt "a photo of a cozy library with warm lighting" at starting indices [1, 3, 5, 7, 10, 20] (left to right), with the original image on the rightmost side. Lower i_start values (left) add more noise, creating more dramatic transformations guided by the prompt. Higher i_start values (right) add less noise, resulting in subtler style changes that preserve more of the original image.</div>
                </div>
            </div>

            <div class="project-section">
                <h2>Part 1.8: Visual Anagrams</h2>
                <p>
                    Visual anagrams are optical illusions where an image looks like one thing when viewed normally, but 
                    reveals a different image when flipped upside down. I implemented this using a technique that 
                    averages noise estimates from two different prompts—one for the normal orientation and one for the 
                    flipped orientation.
                </p>
                <p>
                    The algorithm:
                </p>
                <ol>
                    <li>Denoises the image normally with prompt $p_1$ to get noise estimate $\epsilon_1$</li>
                    <li>Flips the image, denoises with prompt $p_2$ to get $\epsilon_2$, then flips back</li>
                    <li>Averages the two noise estimates: $\epsilon = \frac{1}{2}(\epsilon_1 + \epsilon_2)$</li>
                    <li>Performs the denoising step with the averaged estimate</li>
                </ol>

                <div class="code-container">
                    <div class="code-title">Visual anagrams implementation (pseudocode)</div>
                    <pre><code>FUNCTION visual_anagrams(starting_image, start_index, prompt1, prompt2,
                         unconditional_prompt, timesteps, cfg_scale=7):
    """
    Create a visual anagram: an image that looks like prompt1 when normal,
    but like prompt2 when flipped upside down.
    
    Input:
        starting_image: Starting image (can be noise or noisy image)
        start_index: Starting index in timesteps
        prompt1: Embeddings for normal orientation prompt
        prompt2: Embeddings for flipped orientation prompt
        unconditional_prompt: Unconditional embeddings
        timesteps: List of timesteps
        cfg_scale: CFG scale
    
    Output:
        anagram_image: Image that creates the anagram effect
    """
    
    current_image = starting_image
    
    FOR i FROM start_index TO length(timesteps) - 2:
        
        t = timesteps[i]
        t' = timesteps[i+1]
        ᾱ_t = lookup_cumulative_alpha(t)
        ᾱ_t' = lookup_cumulative_alpha(t')
        α_t = ᾱ_t / ᾱ_t'
        β_t = 1 - α_t
        
        // COMPUTE NOISE ESTIMATE 1: Normal orientation with prompt1
        cond_output_1 = diffusion_model(current_image, t, prompt1)
        uncond_output_1 = diffusion_model(current_image, t, unconditional_prompt)
        
        ε_1_cond = extract_noise_estimate(cond_output_1)
        ε_1_uncond = extract_noise_estimate(uncond_output_1)
        variance = extract_variance_estimate(cond_output_1)
        
        ε_1 = ε_1_uncond + cfg_scale · (ε_1_cond - ε_1_uncond)
        
        // COMPUTE NOISE ESTIMATE 2: Flipped orientation with prompt2
        flipped_image = flip_vertically(current_image)
        
        cond_output_2 = diffusion_model(flipped_image, t, prompt2)
        uncond_output_2 = diffusion_model(flipped_image, t, unconditional_prompt)
        
        ε_2_cond = extract_noise_estimate(cond_output_2)
        ε_2_uncond = extract_noise_estimate(uncond_output_2)
        ε_2_flipped = ε_2_uncond + cfg_scale · (ε_2_cond - ε_2_uncond)
        
        // FLIP BACK to normal orientation
        ε_2 = flip_vertically(ε_2_flipped)
        
        // AVERAGE the two noise estimates
        ε = 0.5 · ε_1 + 0.5 · ε_2
        
        // DENOISE using averaged noise estimate
        x₀_estimate = (current_image - √(1 - ᾱ_t) · ε) / √(ᾱ_t)
        x₀_coeff = √(ᾱ_t') · β_t / (1 - ᾱ_t)
        x_t_coeff = √(α_t) · (1 - ᾱ_t') / (1 - ᾱ_t)
        
        next_image = x₀_coeff · x₀_estimate + x_t_coeff · current_image
        next_image = add_variance(variance, t, next_image)
        
        current_image = next_image
    
    RETURN current_image</code></pre>
                </div>

                <p>
                    I created two visual anagrams:
                </p>
                <ol>
                    <li><strong>Balloon/Lighthouse:</strong> "a watercolor painting of a hot air balloon" (normal) and "a lithograph of a lighthouse on a cliff" (flipped)</li>
                    <li><strong>Farmers/Sea:</strong> "an oil painting of a bustling farmer's market" (normal) and "an oil painting of a stormy sea" (flipped)</li>
                </ol>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/balloon_lighthouse_anagram.jpg" alt="Balloon lighthouse anagram">
                    <div class="project-caption">Balloon lighthouse anagram: When viewed normally, the image shows "a watercolor painting of a hot air balloon". When flipped upside down, it reveals "a lithograph of a lighthouse on a cliff".</div>
                </div>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/farmers_sea_anagram.jpg" alt="Farmers sea anagram">
                    <div class="project-caption">Farmers sea anagram: Normal orientation shows "an oil painting of a bustling farmer's market", while the flipped version reveals "an oil painting of a stormy sea".</div>
                </div>
            </div>

            <div class="project-section">
                <h2>Part 1.9: Hybrid Images</h2>
                <p>
                    Hybrid images combine two different prompts by mixing their frequency components, similar to the 
                    hybrid images from Project 2. I implemented Factorized Diffusion, which creates a composite noise 
                    estimate by combining low frequencies from one prompt with high frequencies from another.
                </p>
                <p>
                    The algorithm:
                </p>
                <ol>
                    <li>Compute noise estimates $\epsilon_1$ and $\epsilon_2$ for prompts $p_1$ and $p_2$</li>
                    <li>Apply Gaussian blur to get low-pass versions: $\epsilon_{1,\text{low}}$, $\epsilon_{2,\text{low}}$</li>
                    <li>Compute high-pass: $\epsilon_{2,\text{high}} = \epsilon_2 - \epsilon_{2,\text{low}}$</li>
                    <li>Combine: $\epsilon = \epsilon_{1,\text{low}} + \epsilon_{2,\text{high}}$</li>
                </ol>

                <div class="code-container">
                    <div class="code-title">Hybrid images implementation (pseudocode)</div>
                    <pre><code>FUNCTION make_hybrids(starting_image, start_index, prompt1, prompt2,
                     unconditional_prompt, timesteps, cfg_scale=7):
    """
    Create hybrid images using Factorized Diffusion.
    Combines low frequencies from prompt1 with high frequencies from prompt2.
    
    Input:
        starting_image: Starting image (typically noise)
        start_index: Starting index in timesteps
        prompt1: Embeddings for low-frequency prompt (visible from far)
        prompt2: Embeddings for high-frequency prompt (visible up close)
        unconditional_prompt: Unconditional embeddings
        timesteps: List of timesteps
        cfg_scale: CFG scale
    
    Output:
        hybrid_image: Hybrid image combining both prompts
    """
    
    current_image = starting_image
    
    FOR i FROM start_index TO length(timesteps) - 2:
        
        t = timesteps[i]
        t' = timesteps[i+1]
        ᾱ_t = lookup_cumulative_alpha(t)
        ᾱ_t' = lookup_cumulative_alpha(t')
        α_t = ᾱ_t / ᾱ_t'
        β_t = 1 - α_t
        
        // COMPUTE NOISE ESTIMATE 1: For prompt1 (low frequencies)
        cond_output_1 = diffusion_model(current_image, t, prompt1)
        uncond_output_1 = diffusion_model(current_image, t, unconditional_prompt)
        
        ε_1_cond = extract_noise_estimate(cond_output_1)
        ε_1_uncond = extract_noise_estimate(uncond_output_1)
        variance = extract_variance_estimate(cond_output_1)
        
        ε_1 = ε_1_uncond + cfg_scale · (ε_1_cond - ε_1_uncond)
        
        // COMPUTE NOISE ESTIMATE 2: For prompt2 (high frequencies)
        cond_output_2 = diffusion_model(current_image, t, prompt2)
        uncond_output_2 = diffusion_model(current_image, t, unconditional_prompt)
        
        ε_2_cond = extract_noise_estimate(cond_output_2)
        ε_2_uncond = extract_noise_estimate(uncond_output_2)
        
        ε_2 = ε_2_uncond + cfg_scale · (ε_2_cond - ε_2_uncond)
        
        // FREQUENCY DECOMPOSITION using Gaussian blur
        // Low-pass filter: blur removes high frequencies
        ε_1_low = apply_gaussian_blur(ε_1, kernel_size=33, sigma=2)
        ε_2_low = apply_gaussian_blur(ε_2, kernel_size=33, sigma=2)
        
        // High-pass filter: subtract low-pass from original
        ε_2_high = ε_2 - ε_2_low
        
        // COMBINE: low frequencies from prompt1, high frequencies from prompt2
        ε = ε_1_low + ε_2_high
        
        // DENOISE using composite noise estimate
        x₀_estimate = (current_image - √(1 - ᾱ_t) · ε) / √(ᾱ_t)
        x₀_coeff = √(ᾱ_t') · β_t / (1 - ᾱ_t)
        x_t_coeff = √(α_t) · (1 - ᾱ_t') / (1 - ᾱ_t)
        
        next_image = x₀_coeff · x₀_estimate + x_t_coeff · current_image
        next_image = add_variance(variance, t, next_image)
        
        current_image = next_image
    
    RETURN current_image</code></pre>
                </div>

                <p>
                    The result is an image that looks like prompt $p_1$ when viewed from far away (low frequencies 
                    dominate) and like prompt $p_2$ when viewed up close (high frequencies dominate).
                </p>
                <p>
                    I created three hybrid images:
                </p>
                <ol>
                    <li><strong>Cat/Meadow:</strong> "a photo of a cat sleeping on a windowsill" (low-pass) and "a watercolor painting of a wildflower meadow" (high-pass)</li>
                    <li><strong>Piano Player/Frost:</strong> "a sketch of a piano player performing" (low-pass) and "a photo of frost on winter branches" (high-pass)</li>
                    <li><strong>Cat/Lighthouse:</strong> "a photo of a cat sleeping on a windowsill" (low-pass) and "a lithograph of a lighthouse on a cliff" (high-pass)</li>
                </ol>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/cat_meadow_hybrid.jpg" alt="Cat meadow hybrid">
                    <div class="project-caption">Cat meadow hybrid: Combining "a photo of a cat sleeping on a windowsill" (low frequencies, visible from far) and "a watercolor painting of a wildflower meadow" (high frequencies, visible up close).</div>
                </div>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/frozen_branch_piano_player.jpg" alt="Piano player frost hybrid">
                    <div class="project-caption">Piano player frost hybrid: Combining "a sketch of a piano player performing" (low frequencies, visible from far away) and "a photo of frost on winter branches" (high frequencies, visible up close). The hybrid image shows the piano player when viewed from a distance, but reveals intricate frost patterns when viewed closely.</div>
                </div>
                <div class="image-container">
                    <img src="project5/proj5a_imgs/cat_lighthouse_hybrid.jpg" alt="Cat lighthouse hybrid">
                    <div class="project-caption">Cat lighthouse hybrid: Combining "a photo of a cat sleeping on a windowsill" (low frequencies) and "a lithograph of a lighthouse on a cliff" (high frequencies).</div>
                </div>
            </div>
        </div>
    </main>
    <script src="../ascii-loader.js"></script>
</body>
</html>

